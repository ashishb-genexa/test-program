{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def get_exact_urls_from_list(list_urls : list, validation_list : str) -> list:\n",
    "\turl_set = set()\n",
    "\tfor url in list_urls:\n",
    "\t\tif url == \"nan\":\n",
    "\t\t\tcontinue\n",
    "\t\tparsed_url = urlparse(url)\n",
    "\t\tlast_section = parsed_url.path.strip('/').split('/')[-1]\n",
    "\t\tfor sub_url in validation_list:\n",
    "\t\t\tif sub_url in str(last_section).strip().lower():\n",
    "\t\t\t\turl_set.add(url)\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn list(url_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import src.webscrapper.webscrapper as wsc\n",
    "import src.util.helper as helper\n",
    "import src.settings.constants as const\n",
    "import src.util.sqllite_helper as db_manager\n",
    "import requests\n",
    "def get_website_child_url(website : str, company_name : str, robot_file,child_url_keywords:list) ->dict:\n",
    "\trsp = {}\n",
    "\trsp[\"is_success\"] = True \n",
    "\trsp[\"child_urls\"] = None\n",
    "\trsp[\"error\"] = \"\"\n",
    "\n",
    "\twebsite = helper.format_url(website)\n",
    "\tstr_sql= \"SELECT child_url FROM child_url_list WHERE company_name = ? AND root_url=?\"\n",
    "\tres = db_manager.select_scaler(str_sql,[company_name,website])\n",
    "\tif res != const.NOT_EXISTS:\n",
    "\t\trsp[\"child_urls\"]= json.loads(res)\n",
    "\t\treturn rsp\n",
    "\t\n",
    "\tif robot_file == const.NOT_FOUND:\n",
    "\t\tis_url_allowed_to_scrap = True\n",
    "\telse:\n",
    "\t\tis_url_allowed_to_scrap = wsc.is_scraping_allowed_by_robots_file(website,robot_file)\n",
    "\n",
    "\tif not is_url_allowed_to_scrap:\n",
    "\t\t#logger.info(f\"url {website} is not allowed scrap by robot.txt file.\")\n",
    "\t\trsp[\"error\"] = \"SCRAPPING_NOT_ALLOWED\"\t\n",
    "\t\trsp[\"is_success\"] = False\n",
    "\t\treturn rsp\t\n",
    "\ttry:\n",
    "\t\theaders = {'User-Agent': const.USER_AGENTS[0]}\n",
    "\t\twith requests.get(website,headers=headers, timeout=const.REQUEST_TIME_OUT,verify=False) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\t\t\tpage_urls = wsc.url_from_html(r.text)\n",
    "\t\t\t#logger.info(f\"Home Page All Urls : {page_urls}\")\n",
    "\t\t\tchild_urls = \",\".join(wsc.validate_page_urls(website,page_urls))\n",
    "\texcept Exception as e:\n",
    "\t\trsp[\"is_success\"] = False\n",
    "\t\trsp[\"error\"] = e\n",
    "\t\t#logger.error(f\"error occured : {e}\")\n",
    "\texact_urls =  str(child_urls).strip().lower().split(\",\")\n",
    "\texact_urls = get_exact_urls_from_list(exact_urls,child_url_keywords)\n",
    "\trsp[\"child_urls\"] = exact_urls\n",
    "\tif rsp[\"is_success\"]:\n",
    "\t\tstr_sql=\"INSERT INTO child_url_list (company_name,root_url, child_url)VALUES (?, ?, ?)\"\n",
    "\t\tdb_manager.execute_sql(str_sql,[company_name, website, json.dumps(exact_urls)])\n",
    "\treturn rsp\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import src.util.helper as helper\n",
    "def get_page_content(page_url:str, robot_file )->dict :\n",
    "\t\"\"\"\n",
    "\tThis Function will go through Website specific page Find page text content.\n",
    "\t\"\"\"\n",
    "\trsp = {}\n",
    "\trsp[\"is_success\"] = False\n",
    "\tstr_sql = \"SELECT page_text from company where page_url= ?\"\n",
    "\tres = db_manager.select_scaler(str_sql,[page_url])\n",
    "\tif res != const.NOT_EXISTS:\n",
    "\t\trsp[\"page_text\"]= res\n",
    "\t\trsp[\"is_success\"] = True\n",
    "\t\treturn rsp\n",
    "\n",
    "\n",
    "\tif robot_file == const.NOT_FOUND:\n",
    "\t\tis_url_allowed_to_scrap = True\n",
    "\telse:\n",
    "\t\tis_url_allowed_to_scrap = wsc.is_scraping_allowed_by_robots_file(page_url,robot_file)\n",
    "\n",
    "\tif not is_url_allowed_to_scrap:\n",
    "\t\t#logger.info(f\"url {page_url} is not allowed scrap by robot.txt file.\")\n",
    "\t\trsp[\"page_text\"] = \"SCRAPPING_NOT_ALLOWED\"\t\n",
    "\t\treturn rsp\n",
    "\t\n",
    "\t#logger.info(f\"Request started for : {page_url}\")\n",
    "\ttry:\n",
    "\t\theaders = {'User-Agent': random.choice(const.USER_AGENTS)}\n",
    "\t\twith requests.get(page_url,headers=headers, timeout=const.REQUEST_TIME_OUT) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\t\t\trsp[\"page_text\"] = helper.clean_scrapped_text(wsc.text_from_html(r.text))\n",
    "\t\t\trsp[\"is_success\"] = True\n",
    "\texcept Exception as e:\n",
    "\t\trsp[\"page_text\"] = \"ERROR\" \n",
    "\t\trsp[\"is_success\"] = False\n",
    "\t\t#logger.error(f\"error occured : {e}\")\n",
    "\t#logger.info(f\"Request Completed for : {page_url}\")\n",
    "\treturn rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retirive_all(data: dict):\n",
    "    for key, value in data.items():\n",
    "        if value == const.NOT_FOUND:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.api_helper as api_helper\n",
    "import src.settings.constants as const\n",
    "def get_comapny_data(website : str, comapny_name : str) ->dict:\n",
    "    #logger.info(f\"Searchin website : {website} for company name : {comapny_name}\")\n",
    "    rsp ={}\n",
    "    url_set=set()\n",
    "    rsp[\"is_success\"]= False\n",
    "    rsp[\"url\"]=[]\n",
    "    rsp[\"error\"]=\"\"\n",
    "    rsp[\"llm_company_name\"]=const.NOT_FOUND\n",
    "    rsp[\"llm_address\"]=const.NOT_FOUND\n",
    "    rsp[\"llm_email\"]=const.NOT_FOUND\n",
    "    rsp[\"llm_phone\"]=const.NOT_FOUND\n",
    "\n",
    "\n",
    "    website = helper.format_url(website)\n",
    "    robot_file = wsc.get_website_robot_file(website, comapny_name)\n",
    "    rsp_url = get_website_child_url(website,comapny_name,robot_file,const.COMPANY_CONACT_ABOUT)\n",
    "\n",
    "    if  not rsp_url[\"is_success\"]  :\n",
    "        rsp['error']= rsp_url['error']\n",
    "        return rsp\n",
    "    \n",
    "    for url in rsp_url['child_urls']:\n",
    "        rsp_page= get_page_content(url,robot_file)\n",
    "        if rsp_page['is_success']:\n",
    "            openAi_rsp = json.loads(api_helper.get_response_from_openai_json(const.PROMPT_DICT[\"compnay\"].replace(\"{context}\",rsp_page['page_text'])))\n",
    "\n",
    "            if openAi_rsp['company_name'] != const.NOT_FOUND:\n",
    "                rsp[\"is_success\"]= True\n",
    "                rsp[\"llm_company_name\"]= openAi_rsp[\"company_name\"]\n",
    "                url_set.add(url)\n",
    "            if openAi_rsp['phone'] != const.NOT_FOUND:\n",
    "                rsp[\"is_success\"]= True\n",
    "                rsp[\"llm_phone\"]= openAi_rsp[\"phone\"]\n",
    "                url_set.add(url)\n",
    "            if openAi_rsp['email'] != const.NOT_FOUND:\n",
    "                rsp[\"is_success\"]= True\n",
    "                rsp[\"llm_email\"]= openAi_rsp[\"email\"]\n",
    "                url_set.add(url)\n",
    "            if openAi_rsp['address'] != const.NOT_FOUND:\n",
    "                rsp[\"is_success\"]= True\n",
    "                rsp[\"llm_address\"]= openAi_rsp[\"address\"]\n",
    "                url_set.add(url)\n",
    "            if is_retirive_all(rsp):\n",
    "                break\n",
    "\n",
    "    rsp[\"url\"]= list(url_set)\n",
    "\n",
    "    if rsp[\"is_success\"]:\n",
    "        str_sql = \"Insert into company (name,page_url,url_type,team_info_json) Values(?,?,?,?)\"\n",
    "        db_manager.execute_sql(str_sql,[rsp[\"llm_company_name\"],json.dumps(rsp['url']),\"contact\",json.dumps(rsp)])\n",
    "    \n",
    "    return rsp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_enrichment.company_data as cenrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = cenrich.get_comapny_data(\"www.a3finance.com\",\"A3 Finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value =cenrich.get_comapny_data(\"https://www.unlbric.com\",\"Big Red Investment Club\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.log_helper as log_helper\n",
    "import src.util.helper as helper\n",
    "import time\n",
    "import os\n",
    "\n",
    "logger = log_helper.set_get_logger(\"contact_deduplication\",helper.get_logfile_name())\n",
    "\n",
    "def get_file_name_and_extension(path):\n",
    "\tfile_name, file_extension = os.path.splitext(os.path.basename(path))\n",
    "\treturn file_name, file_extension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_value(row : any,col_list : list) -> str:\n",
    "\tval_list = []\n",
    "\tfor col in col_list:\n",
    "\t\tval_list.append(str(row[col]).strip())\n",
    "\treturn \" \".join(val_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.similarity_helper as simi_helper\n",
    "\n",
    "def compare_rows(src_row, trg_row):\n",
    "\t\"\"\"\n",
    "\tBased On Compare Columns, Calculate Levenstine, Jaro Winkle and Fuzzy raito and select\n",
    "\twhere Error rate lowest or similarity ratio is higher\n",
    "\t\"\"\"\n",
    "\tcomparesion_list = []\n",
    "\tid = 0\n",
    "\tmin_error = 100.0\n",
    "\tmin_error_id = -1\n",
    "\tfor comp_columns in const.COMPARE_COLUMNS:\n",
    "\t\tstr_src = get_col_value(src_row, comp_columns['src_cols'])\n",
    "\t\tstr_trg = get_col_value(trg_row, comp_columns['trg_cols'])\n",
    "\t\tres = {}\n",
    "\t\tres[\"name\"] = comp_columns[\"name\"]\n",
    "\t\tres[\"f\" + str(id)] = simi_helper.get_fuzzy_similarity(str_src, str_trg)\n",
    "\t\tres[\"l\" + str(id)] = simi_helper.get_levenshtein_similarity(str_src, str_trg)\n",
    "\t\tres[\"j\" + str(id)] = simi_helper.get_jaro_winkler_similarity(str_src, str_trg)\n",
    "\t\t# logger.info(f\"src val : {str_src}, trg val : {str_trg}, fuzz : {res['f' + str(id)] }, levst : {res['l' + str(id)]}, jero : {res['j' + str(id)]}\")\n",
    "\t\tres[\"e\" + str(id)] = (1.0 - (float(res[\"f\" + str(id)]))*0.15 + float(res[\"l\" + str(id)])*0.15 + float(res[\"j\" + str(id)])*0.70) * 100\n",
    "\t\tif float(res[\"e\" + str(id)]) < min_error:\n",
    "\t\t\tmin_error = res[\"e\" + str(id)]\n",
    "\t\t\tmin_error_id = id\n",
    "\t\tcomparesion_list.append(res)\n",
    "\t\tid += 1\n",
    "\t# logger.info(f\"min error rate : {min_error}, min error id : {min_error_id}\")\n",
    "\treturn min_error_id, comparesion_list[min_error_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dup_row_by_sequence(df, sort_col):\n",
    "\tdf = df.sort_values(by=[sort_col])\n",
    "\tlist_dup_rows = []\n",
    "\t# Now Iterate through Each record and try to find out Group\n",
    "\tfor src_idx in range(len(df)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "\t\tfor trg_idx in range(len(df)):\n",
    "\t\t\ttrg_row = df.iloc[trg_idx]\n",
    "\t\t\t# logger.info(f\"source index {src_idx} ,target index : {trg_idx}\")\n",
    "\t\t\tif trg_idx > src_idx:\n",
    "\t\t\t\tdup_grp = {}\n",
    "\t\t\t\tdup_grp[const.COL_SOURCE_ID] = src_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[const.COL_TARGET_ID] = trg_row[const.COL_ID]\n",
    "\t\t\t\tid, rsp = compare_rows(src_row, trg_row)\n",
    "\t\t\t\t# logger.info(id, rsp)\n",
    "\t\t\t\tdup_grp[const.COL_FUZZ_SIMILARITY] = float(rsp[\"f\" + str(id)])\n",
    "\t\t\t\tdf[const.COL_LEVENSHTEIN_SIMILARITY]  = float(rsp[\"l\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_JARO_SIMILARITY] = float(rsp[\"j\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_ERROR_RATE] = float(rsp[\"e\" + str(id)])\n",
    "\t\t\t\tif dup_grp[const.COL_ERROR_RATE] <= const.MAX_ERROR_RATE:\n",
    "\t\t\t\t\tlist_dup_rows.append(dup_grp)\n",
    "\treturn list_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_dup_row_by_fname_lname(df, col_first_name=\"FirstName\"):\n",
    "\tlist_dup_rows = []\n",
    "\t# Group by Fname first.\n",
    "\tdf_dup = df.groupby([\"fname\"], as_index=False).agg(count=(\"fname\", 'count'))\n",
    "\tdf_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "\tfor id, row in df_dup.iterrows():\n",
    "\t\tdf_flt = df[df['fname'] == row['fname']]\n",
    "\t\tdf_flt = df_flt.sort_values(by=[col_first_name])\n",
    "\t\t\n",
    "\t\tlogger.info(f\"Started processing records start with {row['fname']}.\")\n",
    "\t\t# Group By lname\n",
    "\t\tdf_dup_lname = df_flt.groupby([\"lname\"], as_index=False).agg(count=(\"lname\", 'count'))\n",
    "\t\tdf_dup_lname = df_dup_lname[df_dup_lname[\"count\"] > 1]\n",
    "\n",
    "\t\t# Now Process through each lname group Records\n",
    "\t\tfor id1, row in df_dup_lname.iterrows():\n",
    "\t\t\tdf_flt_lname = df_flt[df_flt['lname'] == row['lname']]\n",
    "\t\t\tdf_flt_lname = df_flt_lname.sort_values(by=[\"LastName\"])\n",
    "\t\t\tlist_dup_rows.extend(find_dup_row_by_sequence(df=df_flt_lname, sort_col=\"LastName\"))\n",
    "\n",
    "\tlogger.info(\"Completed processing all records.\")\n",
    "\tdf_dup_rows = pd.DataFrame.from_dict(list_dup_rows)\n",
    "\treturn df_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file_prep_dataframe(file_path, file_ext, map_file_path):\n",
    "\tif file_ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path, encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path)\n",
    "\n",
    "\t# Reading Mapping file prep dictionary\n",
    "\tdf_map = pd.read_excel(map_file_path)\n",
    "\tif \"SRC_COL\" not in df_map.columns or \"TRG_COL\" not in df_map.columns:\n",
    "\t\tlogger.error(\"Mapping file does not have 'SRC_COL' or 'TRG_COL'. Please correct file and re run.\")\n",
    "\t\treturn\n",
    "\n",
    "\tfor id, row in df_map.iterrows():\n",
    "\t\tsrc_col = str(row[\"SRC_COL\"]).strip()\n",
    "\t\ttrg_col = str(row[\"TRG_COL\"]).strip()\n",
    "\t\tif row[\"TRG_COL\"] not in df.columns:\n",
    "\t\t\tlogger.error(f\"Target column {trg_col} does not exists into input file {file_path}. Please correct mapping file and re run.\")\n",
    "\t\t\treturn\n",
    "\t\t# Add Column\n",
    "\t\tif trg_col.lower() == src_col.lower():\n",
    "\t\t\tdf.rename(columns={trg_col: trg_col + \"_org\"}, inplace=True)\n",
    "\t\t\tdf[src_col] = df[trg_col + \"_org\"]\n",
    "\t\telse:\n",
    "\t\t\tdf[src_col] = df[trg_col]\n",
    "\t\t# Clean only SRC Columns\n",
    "\t\tdf[src_col] = df[src_col].map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "\tdf = df.fillna(\"\")  # Fill Empty value as all columns are strings only\n",
    "\n",
    "\t# Clean text of all dataframe text data one time\n",
    "\t# This is not required now. Need to do during export the file.\n",
    "\t# df = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "\t## Add New Columns to Add into original Dataframe\n",
    "\tdf.insert(0, const.COL_DUP_GROUP_ID, 999999)\n",
    "\tif const.COL_ID not in df.columns:\n",
    "\t\t# Add RowNum to DataFrame\n",
    "\t\tdf.insert(1, const.COL_ID, range(0 + 1, len(df) + 1))\n",
    "\telse:\n",
    "\t\tdf.rename(columns={const.COL_ID: const.COL_ID + \"_org\"}, inplace=True)\n",
    "\t\tdf.insert(1, const.COL_ID, range(0 + 1, len(df) + 1))\n",
    "\n",
    "\tdf.insert(2, const.COL_ERROR_RATE, 100.00)\n",
    "\tdf[const.COL_FUZZ_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_LEVENSHTEIN_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_JARO_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_DUP_ROW_GROUP] = \"\"\n",
    "\n",
    "\t# Inserting new column from First Name, take first letter of first name\n",
    "\tdf['fname'] = df[\"FirstName\"].apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "\t#df['lname'] = df[\"LastName\"].apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "\tdf['lname'] = df[\"LastName\"].apply(lambda x: x[:2] if isinstance(x, str) else '')\n",
    "\n",
    "\t# Inserting new column from First Name, take first letter of first name\n",
    "\tdf['part_email'] = df[\"Email\"].apply(lambda x: x.split('@')[0])\n",
    "\t# Add new columns required\n",
    "\tdf[\"dup_group_type\"] = \"\"\n",
    "\tdf[\"data_source\"] = \"\"\n",
    "\tdf[\"action\"] = \"\"\n",
    "\tdf[\"source\"] = \"\"\n",
    "\tdf[\"new_title\"] = \"\"\n",
    "\tdf[\"new_phone\"] = \"\"\n",
    "\tdf[\"new_email\"] = \"\"\n",
    "\tdf[\"prv_org\"] = \"\"\n",
    "\tdf[\"prv_title\"] = \"\"\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_file(file_path : str, ext:str):\n",
    "\tif ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path, encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_group(row, row_to_group, group_id):\n",
    "\tif row in row_to_group:\n",
    "\t\treturn row_to_group[row]\n",
    "\trow_to_group[row] = group_id\n",
    "\treturn group_id\n",
    "\n",
    "def union_groups(row1, row2, row_to_group, group_id):\n",
    "\tgroup1 = find_group(row1, row_to_group, group_id)\n",
    "\tgroup2 = find_group(row2, row_to_group, group_id)\n",
    "\tif group1 != group2:\n",
    "\t\tfor row in row_to_group:\n",
    "\t\t\tif row_to_group[row] == group2:\n",
    "\t\t\t\trow_to_group[row] = group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_group_ids(pairs):\n",
    "\trow_to_group = {}\n",
    "\tgroup_id = 1\n",
    "\n",
    "\tfor src_row, trg_row in pairs:\n",
    "\t\tif src_row not in row_to_group and trg_row not in row_to_group:\n",
    "\t\t\trow_to_group[src_row] = group_id\n",
    "\t\t\trow_to_group[trg_row] = group_id\n",
    "\t\t\tgroup_id += 1\n",
    "\t\telif src_row in row_to_group and trg_row not in row_to_group:\n",
    "\t\t\trow_to_group[trg_row] = row_to_group[src_row]\n",
    "\t\telif trg_row in row_to_group and src_row not in row_to_group:\n",
    "\t\t\trow_to_group[src_row] = row_to_group[trg_row]\n",
    "\t\telse:\n",
    "\t\t\tunion_groups(src_row, trg_row, row_to_group, group_id)\n",
    "\n",
    "\t# Ensure each component has a unique group ID\n",
    "\tunique_groups = {}\n",
    "\tcurrent_group_id = 1\n",
    "\tfor row in row_to_group:\n",
    "\t\told_group_id = row_to_group[row]\n",
    "\t\tif old_group_id not in unique_groups:\n",
    "\t\t\tunique_groups[old_group_id] = current_group_id\n",
    "\t\t\tcurrent_group_id += 1\n",
    "\t\trow_to_group[row] = unique_groups[old_group_id]\n",
    "\n",
    "\treturn row_to_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.settings.constants as const\n",
    "\n",
    "def assign_dup_row_groups(df, dup_df):\n",
    "\tdup_df = dup_df.sort_values(by=[\"src_row_no\"])\n",
    "\n",
    "\t# This will give Group ID assign to each row\n",
    "\tgroup_ids = assign_group_ids(list(dup_df[['src_row_no', 'trg_row_no']].itertuples(index=False, name=None)))\n",
    "\n",
    "\t# In this Loop If any two record qualify or three records, One Record data is missing.\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\n",
    "\t\tif df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0] == 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[const.COL_FUZZ_SIMILARITY], row[const.COL_LEVENSHTEIN_SIMILARITY], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], group_ids[row[const.COL_SOURCE_ID]], str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[const.COL_FUZZ_SIMILARITY], row[const.COL_LEVENSHTEIN_SIMILARITY], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], group_ids[row[const.COL_TARGET_ID]], str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\n",
    "\t# Loop Through again with Dup Record for any Source Or Target Row is missing For Group ID Update\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\tif grp_id != 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[const.COL_FUZZ_SIMILARITY], row[const.COL_LEVENSHTEIN_SIMILARITY], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], grp_id, str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_TARGET_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[const.COL_FUZZ_SIMILARITY], row[const.COL_LEVENSHTEIN_SIMILARITY], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], grp_id, str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_contact_deduplication(file_path: str, map_file_path: str):\n",
    "\tstart_time = time.time()\n",
    "\ttry:\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\tlogger.error(f\"File {file_path} does not exist. Please check file path and try again.\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif not os.path.exists(map_file_path):\n",
    "\t\t\tlogger.error(f\"Mapping file {map_file_path} does not exist. Please check file path and try again.\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tname, ext = helper.get_file_name_and_extension(file_path)\n",
    "\t\tlogger.info(f\"input file name: {name}{ext}\")\n",
    "\t\tif not (ext.lower() == \".csv\" or ext.lower() == \".xlsx\"):\n",
    "\t\t\tlogger.error(\"System supports csv and xlsx file types only. Please check file extension and try again.\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Step 1\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" + name + \"_org_nGram\" + ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\tlogger.info(f\"Started processing file {file_path}\")\n",
    "\t\t\tdf = read_file_prep_dataframe(file_path, ext, map_file_path)\n",
    "\t\t\tlogger.info(f\"No Of Records {len(df)} found in file {file_path}\")\n",
    "\t\t\t# return df\n",
    "\t\t\tlogger.info(f\"Saving original file {file_out}\")\n",
    "\t\t\thelper.save_file(df, file_out, ext)\n",
    "\t\telse:\n",
    "\t\t\tlogger.info(f\"original file {file_out} exists, It will use it\")\n",
    "\t\t\tdf = helper.load_file(file_out, ext)\n",
    "\t\t\t# return df\n",
    "\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" + name + \"_dup_N_gram\" + ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\t# Step 2\n",
    "\t\t\tlogger.info(\"Started finding probable and exact duplicate records\")\n",
    "\t\t\tdf_dup = find_dup_row_by_fname_lname(df)\n",
    "\t\t\tif len(df_dup) == 0:\n",
    "\t\t\t\tlogger.info(\"There are no duplicate rows found.\")\n",
    "\t\t\t\treturn df\n",
    "\t\t\telse:\n",
    "\t\t\t\tlogger.info(f\"No Of Duplicate Records found: {len(df_dup)}\")\n",
    "\n",
    "\t\t\t# Step 3\n",
    "\t\t\tlogger.info(\"Assigning duplicate group id to duplicate rows\")\n",
    "\t\t\tdf = assign_dup_row_groups(df, df_dup)\n",
    "\t\t\tdf = df.fillna(\"\")\n",
    "\t\t\tlogger.info(f\"Saving file with duplicate groups file {file_out}\")\n",
    "\t\t\thelper.save_file(df, file_out, ext)\n",
    "\t\telse:\n",
    "\t\t\tlogger.info(f\"duplicate group file {file_out} exists, It will use it.\")\n",
    "\t\t\tdf = helper.load_file(file_out, ext)\n",
    "\texcept Exception as e:\n",
    "\t\tlogger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_contact_deduplication(\"./data/Contact_03222021.xlsx\",\"./data/Contact_03222021_Mapping.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_contact_deduplication(\"./data/Contact_03222021.xlsx\",\"./data/Contact_03222021_Mapping.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Algorithems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.similarity_helper as simi_helper\n",
    "import time\n",
    "def compare_rows(src_row, trg_row):\n",
    "\t\"\"\"\n",
    "\tBased On Compare Columns, Calculate Levenstine, Jaro Winkle and Fuzzy raito and select\n",
    "\twhere Error rate lowest or similarity ratio is higher\n",
    "\t\"\"\"\n",
    "\tcomparesion_list = []\n",
    "\tid = 0\n",
    "\tmin_error = 100.0\n",
    "\tmin_error_id = -1\n",
    "\tfor comp_columns in const.COMPARE_COLUMNS:\n",
    "\t\tstr_src = get_col_value(src_row, comp_columns['src_cols'])\n",
    "\t\tstr_trg = get_col_value(trg_row, comp_columns['trg_cols'])\n",
    "\t\tres = {}\n",
    "\t\tres[\"name\"] = comp_columns[\"name\"]\n",
    "\t\tres[\"f\" + str(id)] = simi_helper.get_fuzzy_similarity(str_src, str_trg)\n",
    "\t\tres[\"n\" + str(id)] = simi_helper.ngram_similarity(str_src, str_trg)\n",
    "\t\tres[\"j\" + str(id)] = simi_helper.get_jaro_winkler_similarity(str_src, str_trg)\n",
    "\t\tres[\"s\" + str(id)] = simi_helper.sound_index(str_src,str_trg)\n",
    "\t\t# logger.info(f\"src val : {str_src}, trg val : {str_trg}, fuzz : {res['f' + str(id)] }, levst : {res['l' + str(id)]}, jero : {res['j' + str(id)]}\")\n",
    "\t\tres[\"e\" + str(id)] = (1.0 - ((float(res[\"f\" + str(id)])*.05) + (float(res[\"n\" + str(id)])*0.35) + (float(res[\"j\" + str(id)])*0.30)+(float(res[\"s\" + str(id)])*0.30)) )*100\n",
    "\t\tif float(res[\"e\" + str(id)]) < min_error:\n",
    "\t\t\tmin_error = res[\"e\" + str(id)]\n",
    "\t\t\tmin_error_id = id\n",
    "\t\tcomparesion_list.append(res)\n",
    "\t\tid += 1\n",
    "\t# logger.info(f\"min error rate : {min_error}, min error id : {min_error_id}\")\n",
    "\treturn min_error_id, comparesion_list[min_error_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dup_row_by_sequence(df, sort_col):\n",
    "\tdf = df.sort_values(by=[sort_col])\n",
    "\tlist_dup_rows = []\n",
    "\t# Now Iterate through Each record and try to find out Group\n",
    "\tfor src_idx in range(len(df)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "\t\tfor trg_idx in range(len(df)):\n",
    "\t\t\ttrg_row = df.iloc[trg_idx]\n",
    "\t\t\t# logger.info(f\"source index {src_idx} ,target index : {trg_idx}\")\n",
    "\t\t\tif trg_idx > src_idx:\n",
    "\t\t\t\tdup_grp = {}\n",
    "\t\t\t\tdup_grp[const.COL_SOURCE_ID] = src_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[const.COL_TARGET_ID] = trg_row[const.COL_ID]\n",
    "\t\t\t\tid, rsp = compare_rows(src_row, trg_row)\n",
    "\t\t\t\t# logger.info(id, rsp)\n",
    "\t\t\t\tdup_grp[const.COL_FUZZ_SIMILARITY] = float(rsp[\"f\" + str(id)])\n",
    "\t\t\t\tdup_grp[\"ngram\"] = float(rsp[\"n\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_JARO_SIMILARITY] = float(rsp[\"j\" + str(id)])\n",
    "\t\t\t\tdup_grp[\"Sound_index\"] = float(rsp[\"s\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_ERROR_RATE] = float(rsp[\"e\" + str(id)])\n",
    "\t\t\t\tif dup_grp[const.COL_ERROR_RATE] <= const.MAX_ERROR_RATE:\n",
    "\t\t\t\t\tlist_dup_rows.append(dup_grp)\n",
    "\treturn list_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_dup_row_by_fname_lname(df, col_first_name=\"FirstName\"):\n",
    "\tlist_dup_rows = []\n",
    "\t# Group by Fname first.\n",
    "\tdf_dup = df.groupby([\"fname\"], as_index=False).agg(count=(\"fname\", 'count'))\n",
    "\tdf_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "\tfor id, row in df_dup.iterrows():\n",
    "\t\tdf_flt = df[df['fname'] == row['fname']]\n",
    "\t\tdf_flt = df_flt.sort_values(by=[col_first_name])\n",
    "\t\t\n",
    "\t\tlogger.info(f\"Started processing records start with {row['fname']}.\")\n",
    "\t\t# Group By lname\n",
    "\t\tdf_dup_lname = df_flt.groupby([\"lname\"], as_index=False).agg(count=(\"lname\", 'count'))\n",
    "\t\tdf_dup_lname = df_dup_lname[df_dup_lname[\"count\"] > 1]\n",
    "\n",
    "\t\t# Now Process through each lname group Records\n",
    "\t\tfor id1, row in df_dup_lname.iterrows():\n",
    "\t\t\tdf_flt_lname = df_flt[df_flt['lname'] == row['lname']]\n",
    "\t\t\tdf_flt_lname = df_flt_lname.sort_values(by=[\"LastName\"])\n",
    "\t\t\tlist_dup_rows.extend(find_dup_row_by_sequence(df=df_flt_lname, sort_col=\"LastName\"))\n",
    "\n",
    "\tlogger.info(\"Completed processing all records.\")\n",
    "\tdf_dup_rows = pd.DataFrame.from_dict(list_dup_rows)\n",
    "\treturn df_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file_prep_dataframe(file_path, file_ext, map_file_path):\n",
    "\tif file_ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path, encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path)\n",
    "\n",
    "\t# Reading Mapping file prep dictionary\n",
    "\tdf_map = pd.read_excel(map_file_path)\n",
    "\tif \"SRC_COL\" not in df_map.columns or \"TRG_COL\" not in df_map.columns:\n",
    "\t\tlogger.error(\"Mapping file does not have 'SRC_COL' or 'TRG_COL'. Please correct file and re run.\")\n",
    "\t\treturn\n",
    "\n",
    "\tfor id, row in df_map.iterrows():\n",
    "\t\tsrc_col = str(row[\"SRC_COL\"]).strip()\n",
    "\t\ttrg_col = str(row[\"TRG_COL\"]).strip()\n",
    "\t\tif row[\"TRG_COL\"] not in df.columns:\n",
    "\t\t\tlogger.error(f\"Target column {trg_col} does not exists into input file {file_path}. Please correct mapping file and re run.\")\n",
    "\t\t\treturn\n",
    "\t\t# Add Column\n",
    "\t\tif trg_col.lower() == src_col.lower():\n",
    "\t\t\tdf.rename(columns={trg_col: trg_col + \"_org\"}, inplace=True)\n",
    "\t\t\tdf[src_col] = df[trg_col + \"_org\"]\n",
    "\t\telse:\n",
    "\t\t\tdf[src_col] = df[trg_col]\n",
    "\t\t# Clean only SRC Columns\n",
    "\t\tdf[src_col] = df[src_col].map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "\tdf = df.fillna(\"\")  # Fill Empty value as all columns are strings only\n",
    "\n",
    "\t# Clean text of all dataframe text data one time\n",
    "\t# This is not required now. Need to do during export the file.\n",
    "\t# df = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "\t## Add New Columns to Add into original Dataframe\n",
    "\tdf.insert(0, const.COL_DUP_GROUP_ID, 999999)\n",
    "\tif const.COL_ID not in df.columns:\n",
    "\t\t# Add RowNum to DataFrame\n",
    "\t\tdf.insert(1, const.COL_ID, range(0 + 1, len(df) + 1))\n",
    "\telse:\n",
    "\t\tdf.rename(columns={const.COL_ID: const.COL_ID + \"_org\"}, inplace=True)\n",
    "\t\tdf.insert(1, const.COL_ID, range(0 + 1, len(df) + 1))\n",
    "\n",
    "\tdf.insert(2, const.COL_ERROR_RATE, 100.00)\n",
    "\tdf[const.COL_FUZZ_SIMILARITY] = 0.0\n",
    "\tdf[\"ngram\"] = 0.0\n",
    "\tdf[const.COL_JARO_SIMILARITY] = 0.0\n",
    "\tdf[\"Sound_index\"]= 0.0\n",
    "\tdf[const.COL_DUP_ROW_GROUP] = \"\"\n",
    "\n",
    "\t# Inserting new column from First Name, take first letter of first name\n",
    "\tdf['fname'] = df[\"FirstName\"].apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "\t# df['lname'] = df[\"LastName\"].apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "\tdf['lname'] = df[\"LastName\"].apply(lambda x: x[:2] if isinstance(x, str) else '')\n",
    "\n",
    "\t# Inserting new column from First Name, take first letter of first name\n",
    "\tdf['part_email'] = df[\"Email\"].apply(lambda x: x.split('@')[0])\n",
    "\t# Add new columns required\n",
    "\tdf[\"dup_group_type\"] = \"\"\n",
    "\tdf[\"data_source\"] = \"\"\n",
    "\tdf[\"action\"] = \"\"\n",
    "\tdf[\"source\"] = \"\"\n",
    "\tdf[\"new_title\"] = \"\"\n",
    "\tdf[\"new_phone\"] = \"\"\n",
    "\tdf[\"new_email\"] = \"\"\n",
    "\tdf[\"prv_org\"] = \"\"\n",
    "\tdf[\"prv_title\"] = \"\"\n",
    "\t\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.settings.constants as const\n",
    "\n",
    "def assign_dup_row_groups(df, dup_df):\n",
    "\tdup_df = dup_df.sort_values(by=[\"src_row_no\"])\n",
    "\n",
    "\t# This will give Group ID assign to each row\n",
    "\tgroup_ids = assign_group_ids(list(dup_df[['src_row_no', 'trg_row_no']].itertuples(index=False, name=None)))\n",
    "\n",
    "\t# In this Loop If any two record qualify or three records, One Record data is missing.\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\n",
    "\t\tif df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0] == 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [\"Sound_index\",const.COL_FUZZ_SIMILARITY, \"ngram\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[\"Sound_index\"],row[const.COL_FUZZ_SIMILARITY], row[\"ngram\"], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], group_ids[row[const.COL_SOURCE_ID]], str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [\"Sound_index\",const.COL_FUZZ_SIMILARITY, \"ngram\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[\"Sound_index\"],row[const.COL_FUZZ_SIMILARITY], row[\"ngram\"], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], group_ids[row[const.COL_TARGET_ID]], str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\n",
    "\t# Loop Through again with Dup Record for any Source Or Target Row is missing For Group ID Update\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\tif grp_id != 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [\"Sound_index\",const.COL_FUZZ_SIMILARITY, \"ngram\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[\"Sound_index\"],row[const.COL_FUZZ_SIMILARITY], row[\"ngram\"], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], grp_id, str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_TARGET_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [\"Sound_index\",const.COL_FUZZ_SIMILARITY, \"ngram\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  const.COL_JARO_SIMILARITY, const.COL_ERROR_RATE, const.COL_DUP_GROUP_ID, const.COL_DUP_ROW_GROUP]] = [\n",
    "\t\t\t\trow[\"Sound_index\"],row[const.COL_FUZZ_SIMILARITY], row[\"ngram\"], row[const.COL_JARO_SIMILARITY],\n",
    "\t\t\t\trow[const.COL_ERROR_RATE], grp_id, str(row[const.COL_SOURCE_ID]) + \"-\" + str(row[const.COL_TARGET_ID])]\n",
    "\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_contact_deduplication(file_path: str, map_file_path: str):\n",
    "\tstart_time = time.time()\n",
    "\ttry:\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\tlogger.error(f\"File {file_path} does not exist. Please check file path and try again.\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif not os.path.exists(map_file_path):\n",
    "\t\t\tlogger.error(f\"Mapping file {map_file_path} does not exist. Please check file path and try again.\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tname, ext = helper.get_file_name_and_extension(file_path)\n",
    "\t\tlogger.info(f\"input file name: {name}{ext}\")\n",
    "\t\tif not (ext.lower() == \".csv\" or ext.lower() == \".xlsx\"):\n",
    "\t\t\tlogger.error(\"System supports csv and xlsx file types only. Please check file extension and try again.\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Step 1\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" + name + \"_org_New_logic\" + ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\tlogger.info(f\"Started processing file {file_path}\")\n",
    "\t\t\tdf = read_file_prep_dataframe(file_path, ext, map_file_path)\n",
    "\t\t\tlogger.info(f\"No Of Records {len(df)} found in file {file_path}\")\n",
    "\t\t\t# return df\n",
    "\t\t\tlogger.info(f\"Saving original file {file_out}\")\n",
    "\t\t\thelper.save_file(df, file_out, ext)\n",
    "\t\telse:\n",
    "\t\t\tlogger.info(f\"original file {file_out} exists, It will use it\")\n",
    "\t\t\tdf = helper.load_file(file_out, ext)\n",
    "\t\t\t# return df\n",
    "\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" + name + \"_dup_New_logic\" + ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\t# Step 2\n",
    "\t\t\tlogger.info(\"Started finding probable and exact duplicate records\")\n",
    "\t\t\tdf_dup = find_dup_row_by_fname_lname(df)\n",
    "\t\t\tif len(df_dup) == 0:\n",
    "\t\t\t\tlogger.info(\"There are no duplicate rows found.\")\n",
    "\t\t\t\treturn df\n",
    "\t\t\telse:\n",
    "\t\t\t\tlogger.info(f\"No Of Duplicate Records found: {len(df_dup)}\")\n",
    "\n",
    "\t\t\t# Step 3\n",
    "\t\t\tlogger.info(\"Assigning duplicate group id to duplicate rows\")\n",
    "\t\t\tdf = assign_dup_row_groups(df, df_dup)\n",
    "\t\t\tdf = df.fillna(\"\")\n",
    "\t\t\tlogger.info(f\"Saving file with duplicate groups file {file_out}\")\n",
    "\t\t\thelper.save_file(df, file_out, ext)\n",
    "\t\telse:\n",
    "\t\t\tlogger.info(f\"duplicate group file {file_out} exists, It will use it.\")\n",
    "\t\t\tdf = helper.load_file(file_out, ext)\n",
    "\texcept Exception as e:\n",
    "\t\tlogger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_contact_deduplication(\"./data/Contact Test_Data_Jan 15-2025-407 records-allscenario.xlsx\",\"./data/Contact Test_Data_Jan 15-2025-407 records-allscenario_Mapping.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish as jfish\n",
    "str_src= \"Adam Friedman\"\n",
    "str_trg = \"Adam Freda\"\n",
    "def sound_index(str_src, str_trg):\n",
    "\tsoundex1 = jfish.soundex(str_src)\n",
    "\tsoundex2 = jfish.soundex(str_trg)\n",
    "\tsoundex_score = 1.0 if soundex1 == soundex2 else 0.0\n",
    "\treturn soundex_score\n",
    "sound_index(str_src,str_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "str_src= \"ajit\"\n",
    "str_trg = \"ankit\"\n",
    "def get_levenshtein_similarity(str_src,str_trg):\n",
    "\t\"\"\"\n",
    "\tLevenshtein distance measures the minimum number of single-character edits required to change one string into another.\n",
    "\t\"\"\"\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn Levenshtein.ratio(str_src, str_trg)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "levestin=get_levenshtein_similarity(str_src,str_trg)\n",
    "levestin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666665"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "str_src= \"Geoffrey\"\n",
    "str_trg = \"Gregory\"\n",
    "def get_fuzzy_similarity(str_src, str_trg):\n",
    "\tif len(str_src) != 0 and len(str_trg) != 0:\n",
    "\t\treturn fuzz.token_set_ratio(str_src, str_trg) / 100\n",
    "\telse:\n",
    "\t\treturn 0.0\n",
    "\n",
    "fuzzy=get_fuzzy_similarity(str_src,str_trg)\n",
    "fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017857142857143"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jellyfish as jfish\n",
    "\n",
    "str_src= \"Geoffrey\"\n",
    "str_trg = \"Gregory\"\n",
    "def get_jaro_winkler_similarity(str_src, str_trg):\n",
    "\tif len(str_src) != 0 and len(str_trg) != 0:\n",
    "\t\treturn jfish.jaro_winkler_similarity(str_src, str_trg)\n",
    "\telse:\n",
    "\t\treturn 0.0\n",
    "\n",
    "jarao=get_jaro_winkler_similarity(str_src, str_trg)\n",
    "jarao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"Robert Walker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 100-(((float(fuzzy))*0.25 + float(levestin)*0.25 + float(jarao)*0.50) * 100)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.0-(float(fuzzy) + float(levestin) + float(jarao) )/3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A‡avusoglu\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    # Normalize the string to its decomposed form (NFKD)\n",
    "    normalized = unicodedata.normalize('NFKD', input_str)\n",
    "    # Build a new string without the accent characters (combining marks)\n",
    "    plain_str = ''.join(\n",
    "        char for char in normalized\n",
    "        if not unicodedata.combining(char)\n",
    "    )\n",
    "    return plain_str\n",
    "\n",
    "# Example usage:\n",
    "french_name = \"Ã‡avusoglu\"\n",
    "plain_name = remove_accents(french_name)\n",
    "print(plain_name)  # Output: \"Francois L'Olonnais\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
