{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact De Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-Oct-08 16:41:38 - contact_deduplication - INFO - input file name : TV_Contact_TestData_v1.xlsx\n",
      "2024-Oct-08 16:41:38 - contact_deduplication - INFO - Started processing file ./data/TV_Contact_TestData_v1.xlsx\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - No Of Records 112 found in file ./data/TV_Contact_TestData_v1.xlsx\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - Saving original file ./output//TV_Contact_TestData_v1_org.xlsx\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - Started finding probable and exact duplicate records\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with a.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with b.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with c.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with d.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with e.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with f.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with g.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with h.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with j.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with k.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with l.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with m.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with p.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with r.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with s.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with t.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Started processing records start with w.\n",
      "2024-Oct-08 16:41:39 - duplicate_finder - INFO - Completed processing all records.\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - No Of Duplicate Records found : 18 \n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - Assigning duplicate group id to duplicate rows\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - Saving file with duplicate groups file ./output//TV_Contact_TestData_v1_dup.xlsx\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - duplicate identification Total Processing Time : 0.83 Seconds\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - Started duplicate resolution.\n",
      "2024-Oct-08 16:41:39 - contact_deduplication - INFO - Total exact duplicate found : 3\n",
      "2024-Oct-08 16:41:39 - duplicate_resolution_google_search - INFO - Started assignment of probable duplicate flag : version V1\n",
      "2024-Oct-08 16:41:39 - WebScrapper - INFO - Started searching for contact jason brein and Email domain francisco.com\n",
      "2024-Oct-08 16:41:39 - WebScrapper - INFO - contact : jason brein, company francisco partners google api serach info found from db.\n",
      "2024-Oct-08 16:41:39 - WebScrapper - INFO - company francisco partners and robot file exists in db.\n",
      "2024-Oct-08 16:41:39 - WebScrapper - INFO - Request started for : https://www.franciscopartners.com/team/jason-brein\n",
      "2024-Oct-08 16:41:41 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - Open Ai response : {'curr_org': 'Francisco Partners', 'curr_title': 'Partner', 'prv_org': 'Elevation Partners', 'prv_title': 'Not found', 'email': 'Not found', 'phone': '+1 (646) 434 1343'}\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - Started searching for contact jaison brein and Email domain francisco.com\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - contact : jaison brein, company francisco partners google api serach info found from db.\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - Started searching for contact lance gasch and Email domain hudson.com\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - contact : lance gasch, company hudson capital advisors google api serach info found from db.\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - company hudson capital advisors and robot file exists in db.\n",
      "2024-Oct-08 16:41:43 - WebScrapper - INFO - Request started for : https://hudson-advisors.com/person/lance-gasch/\n",
      "2024-Oct-08 16:41:47 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Open Ai response : {'curr_org': 'hudson', 'curr_title': 'global head of real estate', 'prv_org': 'gramercy capital', 'prv_title': 'Not found', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Started searching for contact lancey gasch and Email domain hudson.com\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - contact : lancey gasch, company hudson capital advisors google api serach info found from db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Started searching for contact lee gros and Email domain brock.com\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - contact : lee gros, company brock google api serach info found from db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - company : brock, url : https://www.brockgroup.com/about_copy/ contact information Not found found in db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - get_linked_profile_by_proxy_curl : code is commented out. PLease uncomment to include it.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - method get_contact_info_using_third_party_db is not implmented yet.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Started searching for contact lee gros and Email domain bridgepointib.com\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - contact : lee gros, company bridgepoint investment banking google api serach info found from db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Started searching for contact marina lombardi and Email domain podiumpartner.com\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - contact : marina lombardi, company mfs capital advisors google api serach info found from db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Started searching for contact marina lombardi and Email domain enel.com\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - contact : marina lombardi, company enel green power google api serach info found from db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - company enel green power and robot file exists in db.\n",
      "2024-Oct-08 16:41:48 - WebScrapper - INFO - Request started for : https://www.enelgreenpower.com/who-we-are/management-team/marina-lombardi\n",
      "2024-Oct-08 16:41:49 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:41:51 - WebScrapper - INFO - Open Ai response : {'curr_org': 'enel green power', 'curr_title': 'head of innovation', 'prv_org': 'enel grids', 'prv_title': 'global head of innovation', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:41:51 - WebScrapper - INFO - Started searching for contact jen pelletier and Email domain hydeparkcapital.com\n",
      "2024-Oct-08 16:41:51 - WebScrapper - INFO - contact : jen pelletier, company hyde park capital google api serach info found from db.\n",
      "2024-Oct-08 16:41:51 - WebScrapper - INFO - company hyde park capital and robot file exists in db.\n",
      "2024-Oct-08 16:41:51 - WebScrapper - INFO - Request started for : https://hydeparkcapital.com/team/jenny-pelletier/\n",
      "2024-Oct-08 16:41:53 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - Open Ai response : {'curr_org': 'hyde park capital', 'curr_title': 'senior executive administrator', 'prv_org': 'Not found', 'prv_title': 'Not found', 'email': 'pelletier@hydeparkcapital.com', 'phone': '(813) 383-0202'}\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - get_linked_profile_by_proxy_curl : code is commented out. PLease uncomment to include it.\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - method get_contact_info_using_third_party_db is not implmented yet.\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - Started searching for contact jenny pelletier and Email domain hydeparkcapital.com\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - contact : jenny pelletier, company hyde park capital google api serach info found from db.\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - company hyde park capital and robot file exists in db.\n",
      "2024-Oct-08 16:41:55 - WebScrapper - INFO - Request started for : https://hydeparkcapital.com/team/jenny-pelletier/\n",
      "2024-Oct-08 16:41:56 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - Open Ai response : {'curr_org': 'hyde park capital', 'curr_title': 'senior executive administrator', 'prv_org': 'Not found', 'prv_title': 'Not found', 'email': 'pelletier@hydeparkcapital.com', 'phone': '(813) 383-0202'}\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - get_linked_profile_by_proxy_curl : code is commented out. PLease uncomment to include it.\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - method get_contact_info_using_third_party_db is not implmented yet.\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - Started searching for contact scott schulte and Email domain podiumpartner.com\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - contact : scott schulte, company mfs capital advisors google api serach info found from db.\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - company mfs capital advisors and robot file exists in db.\n",
      "2024-Oct-08 16:41:59 - WebScrapper - INFO - Request started for : https://www.podiumpartner.com/team\n",
      "2024-Oct-08 16:42:00 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:02 - WebScrapper - INFO - Open Ai response : {'curr_org': 'podium partners', 'curr_title': 'founding partner', 'prv_org': 'morgan stanley', 'prv_title': 'vice president of sales', 'email': 'scott@podiumpartner.com', 'phone': '(917) 562-9610'}\n",
      "2024-Oct-08 16:42:02 - WebScrapper - INFO - Started searching for contact scott schulte and Email domain podiumpartner.com\n",
      "2024-Oct-08 16:42:02 - WebScrapper - INFO - contact : scott schulte, company mfs capital advisors google api serach info found from db.\n",
      "2024-Oct-08 16:42:02 - WebScrapper - INFO - company mfs capital advisors and robot file exists in db.\n",
      "2024-Oct-08 16:42:02 - WebScrapper - INFO - Request started for : https://www.podiumpartner.com/team\n",
      "2024-Oct-08 16:42:03 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - Open Ai response : {'curr_org': 'podium partners', 'curr_title': 'founding partner', 'prv_org': 'morgan stanley', 'prv_title': 'vice president of sales', 'email': 'scott@podiumpartner.com', 'phone': '(917) 562-9610'}\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - Started searching for contact ken sexton and Email domain encompass.com\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - contact : ken sexton, company encompass digital media google api serach info found from db.\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - Started searching for contact kenny sexton and Email domain encompass.com\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - contact : kenny sexton, company encompass digital media google api serach info found from db.\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - Started searching for contact deepak shah and Email domain francisco.com\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - contact : deepak shah, company francisco partners google api serach info found from db.\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - Started searching for contact deep shah and Email domain francisco.com\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - contact : deep shah, company francisco partners google api serach info found from db.\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - company francisco partners and robot file exists in db.\n",
      "2024-Oct-08 16:42:05 - WebScrapper - INFO - Request started for : https://www.franciscopartners.com/team/deep-shah\n",
      "2024-Oct-08 16:42:07 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - Open Ai response : {'curr_org': 'Francisco Partners', 'curr_title': 'Co-President', 'prv_org': 'Morgan Stanley', 'prv_title': 'Various roles across investment banking', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - Started searching for contact ricky thomas and Email domain focusbankers.com\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - contact : ricky thomas, company focus investment banking google api serach info found from db.\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - Started searching for contact rick thomas and Email domain focusbankers.com\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - contact : rick thomas, company focus investment banking google api serach info found from db.\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - company focus investment banking and robot file exists in db.\n",
      "2024-Oct-08 16:42:09 - WebScrapper - INFO - Request started for : https://focusbankers.com/teammember/rick-thomas/\n",
      "2024-Oct-08 16:42:10 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - Open Ai response : {'curr_org': 'Focus Investment Banking LLC', 'curr_title': 'CEO', 'prv_org': 'VSI', 'prv_title': 'Vice President', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - Started searching for contact jeffy tiani and Email domain five.com\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - contact : jeffy tiani, company five point capital google api serach info found from db.\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - Started searching for contact jeff tiani and Email domain five point capital\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - contact : jeff tiani, company five point capital google api serach info found from db.\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - Started searching for contact liu yuejin and Email domain chinalife.com\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - contact : liu yuejin, company china life insurance co.,ltd google api serach info found from db.\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - company china life insurance co.,ltd and robot file exists in db.\n",
      "2024-Oct-08 16:42:13 - WebScrapper - INFO - Request started for : https://www.chinalife.com.hk/about-us/clio\n",
      "2024-Oct-08 16:42:14 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:16 - WebScrapper - INFO - Open Ai response : {'curr_org': 'china life insurance (overseas) company limited', 'curr_title': 'president', 'prv_org': 'china life insurance company limited', 'prv_title': 'assistant to the president', 'email': 'Not found', 'phone': '(852) 3999 5519'}\n",
      "2024-Oct-08 16:42:16 - WebScrapper - INFO - Started searching for contact li yuejin and Email domain chinalife.com\n",
      "2024-Oct-08 16:42:16 - WebScrapper - INFO - contact : li yuejin, company china life insurance co.,ltd google api serach info found from db.\n",
      "2024-Oct-08 16:42:16 - WebScrapper - INFO - company china life insurance co.,ltd and robot file exists in db.\n",
      "2024-Oct-08 16:42:16 - WebScrapper - INFO - Request started for : https://www.chinalife.com.hk/about-us/clio\n",
      "2024-Oct-08 16:42:17 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:18 - WebScrapper - INFO - Open Ai response : {'curr_org': 'china life insurance (overseas) company limited', 'curr_title': 'president', 'prv_org': 'china life insurance company limited', 'prv_title': 'assistant to the president', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:42:18 - WebScrapper - INFO - Started searching for contact ben valics and Email domain podiumpartner.com\n",
      "2024-Oct-08 16:42:18 - WebScrapper - INFO - contact : ben valics, company  google api serach info found from db.\n",
      "2024-Oct-08 16:42:18 - WebScrapper - INFO - Started searching for contact bence valics and Email domain podiumpartner.com\n",
      "2024-Oct-08 16:42:18 - WebScrapper - INFO - contact : bence valics, company mfs capital advisors google api serach info found from db.\n",
      "2024-Oct-08 16:42:18 - WebScrapper - INFO - company mfs capital advisors and robot file exists in db.\n",
      "2024-Oct-08 16:42:19 - WebScrapper - INFO - Request started for : https://www.podiumpartner.com/team\n",
      "2024-Oct-08 16:42:19 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:21 - WebScrapper - INFO - Open Ai response : {'curr_org': 'podium partners', 'curr_title': 'founding partner', 'prv_org': 'morgan stanley', 'prv_title': 'vice president of sales', 'email': 'scott@podiumpartner.com', 'phone': '(917) 562-9610'}\n",
      "2024-Oct-08 16:42:21 - WebScrapper - INFO - Started searching for contact amy south and Email domain cibc.com\n",
      "2024-Oct-08 16:42:21 - WebScrapper - INFO - contact : amy south, company cibc world markets plc google api serach info found from db.\n",
      "2024-Oct-08 16:42:21 - WebScrapper - INFO - company cibc world markets plc and robot file exists in db.\n",
      "2024-Oct-08 16:42:21 - WebScrapper - INFO - Request started for : https://cibccm.com/en/about-us/leadership/amy-south/\n",
      "2024-Oct-08 16:42:25 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:27 - WebScrapper - INFO - Open Ai response : {'curr_org': 'cibc capital markets', 'curr_title': 'executive vice-president, chief administration officer', 'prv_org': 'another financial institution', 'prv_title': 'chief financial officer for investor and treasury services', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:42:27 - WebScrapper - INFO - Started searching for contact a south and Email domain cibc.com\n",
      "2024-Oct-08 16:42:27 - WebScrapper - INFO - contact : a south, company  google api serach info found from db.\n",
      "2024-Oct-08 16:42:27 - WebScrapper - INFO - company  and robot file exists in db.\n",
      "2024-Oct-08 16:42:27 - WebScrapper - INFO - Request started for : https://cibccm.com/en/about-us/leadership/amy-south/\n",
      "2024-Oct-08 16:42:30 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - Open Ai response : {'curr_org': 'cibc capital markets', 'curr_title': 'executive vice-president, chief administration officer', 'prv_org': 'another financial institution', 'prv_title': 'chief financial officer for investor and treasury services', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - Started searching for contact don susca and Email domain enel.com\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - contact : don susca, company green power google api serach info found from db.\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - Started searching for contact donata susca and Email domain enel.com\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - contact : donata susca, company enel green power google api serach info found from db.\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - company enel green power and robot file exists in db.\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - Request started for : https://www.enelgreenpower.com/who-we-are/management-team/donata-susca\n",
      "2024-Oct-08 16:42:32 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:34 - WebScrapper - INFO - Open Ai response : {'curr_org': 'enel green power & thermal generation', 'curr_title': 'global head of health, safety, environment and quality', 'prv_org': 'heera', 'prv_title': 'organization expert', 'email': 'Not found', 'phone': 'Not found'}\n",
      "2024-Oct-08 16:42:34 - WebScrapper - INFO - Started searching for contact joey talocka and Email domain hydeparkcapital.com\n",
      "2024-Oct-08 16:42:34 - WebScrapper - INFO - contact : joey talocka, company hyde park capital google api serach info found from db.\n",
      "2024-Oct-08 16:42:34 - WebScrapper - INFO - company hyde park capital and robot file exists in db.\n",
      "2024-Oct-08 16:42:34 - WebScrapper - INFO - Request started for : https://hydeparkcapital.com/team/joey-talocka/\n",
      "2024-Oct-08 16:42:36 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:38 - WebScrapper - INFO - Open Ai response : {'curr_org': 'hyde park capital', 'curr_title': 'analyst', 'prv_org': 'seamless.ai', 'prv_title': 'senior sales development representative', 'email': 'talocka@hydeparkcapital.com', 'phone': '(813) 383-0202'}\n",
      "2024-Oct-08 16:42:38 - WebScrapper - INFO - Started searching for contact jo talocka and Email domain hydeparkcapital.com\n",
      "2024-Oct-08 16:42:38 - WebScrapper - INFO - contact : jo talocka, company hyde park capitals google api serach info found from db.\n",
      "2024-Oct-08 16:42:38 - WebScrapper - INFO - company hyde park capitals and robot file exists in db.\n",
      "2024-Oct-08 16:42:38 - WebScrapper - INFO - Request started for : https://hydeparkcapital.com/team/joey-talocka/\n",
      "2024-Oct-08 16:42:39 - WebScrapper - INFO - Started searchin using Open AI\n",
      "2024-Oct-08 16:42:41 - WebScrapper - INFO - Open Ai response : {'curr_org': 'Hyde Park Capital', 'curr_title': 'Analyst', 'prv_org': 'Seamless.ai', 'prv_title': 'Senior Sales Development Representative', 'email': 'talocka@hydeparkcapital.com', 'phone': '(813) 383-0202'}\n",
      "2024-Oct-08 16:42:41 - duplicate_resolution_google_search - INFO - row no : 52, keep prv org : enel grids, curr org : mfs capital advisors, Similarity score : 26.66666666666667\n",
      "2024-Oct-08 16:42:41 - duplicate_resolution_google_search - INFO - row no : 108, keep prv org : Morgan Stanley, curr org : francisco partners, Similarity score : 43.75\n",
      "2024-Oct-08 16:42:41 - duplicate_resolution_google_search - INFO - completed assignment of probable duplicate flag\n",
      "2024-Oct-08 16:42:41 - contact_deduplication - INFO - Saving file with duplicate groups and Keep delete flaf ./output//TV_Contact_TestData_v1_with_flag.xlsx\n",
      "2024-Oct-08 16:42:41 - contact_deduplication - INFO - duplicate resolution Total Processing Time : 61.46 Seconds\n"
     ]
    }
   ],
   "source": [
    "import src.contact_deduplication as cdup\n",
    "\n",
    "cdup.process_contact_deduplication(\"./data/TV_Contact_TestData_v1.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Probable duplicates With Google API, OpenAI and Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.contact_deduplication as cd\n",
    "\n",
    "file_path = \"./data/TV_Contact_Probable_Duplicate.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "df.fillna(\"\")\n",
    "df = cd.process_duplicate_resolution(df)\n",
    "df.to_excel(\"./output/TV_Contact_Probable_Duplicate_flag.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Robots File Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.duplicate_finder.DuplicateFinder as dup_find\n",
    "import src.webscrapper.webscrapper as wsc\n",
    "\n",
    "file_path = \"./output/company_robots_stats.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "\n",
    "list_new_flag = []\n",
    "for idx in range(len(df)):\n",
    "  row = df.iloc[idx]\n",
    "  if row[\"is_scraping_allowed\"] == 1:\n",
    "    list_new_flag.append(True)\n",
    "    continue\n",
    "  list_new_flag.append(wsc.is_scraping_allowed_by_robots_file(row[\"website\"],str(row[\"robot_file\"])))      \n",
    "\n",
    "df[\"new_flag_scraping_allowed\"] = list_new_flag\n",
    "df.to_excel(\"./output/company_robots_stats_with_new_flag.xlsx\")\n",
    "\n",
    "# from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# robo_file = \"\"\"\n",
    "# User-agent: *\n",
    "# Crawl-delay: 10\n",
    "# Disallow: /people/people-directory\n",
    "# #Disallow: /people/\n",
    "# Disallow: /search\n",
    "# Disallow: /umbraco/Surface/HtmlToPdfConversion/\n",
    "# Disallow: /umbraco/Surface/VcfDownload/\n",
    "# Disallow: /umbraco\n",
    "# Disallow: /news-insights/in-the-media/\n",
    "# Sitemap: https://www.lowenstein.com/sitemap.xml\n",
    "# \"\"\"\n",
    "# url = \"https://www.lowenstein.com/umbraco/Surface/VcfDownload\"\n",
    "# rp = RobotFileParser()\n",
    "# # rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n",
    "# # rp.read()\n",
    "# rp.parse(robo_file.splitlines())\n",
    "# #print(robo_file.splitlines())\n",
    "\n",
    "# print(rp.can_fetch(\"*\",url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Read / Write using SqlLite Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.sqllite_helper as dbmgr\n",
    "my_dict = {\"name\":\"Test\",\"page_url\":\"www.test.com\",\"url_type\": \"root\",\"page_text\":\"sample text, not found\",\"is_error\":\"False\" }\n",
    "\n",
    "str_sql = \"Insert into company_website_pages ( name, page_url,url_type,page_text,is_error) Values(?,?,?,?,?)\"\n",
    "data = tuple(my_dict.values())\n",
    "# rsp = dbmgr.execute_sql(str_sql,data)\n",
    "# print(rsp)\n",
    "\n",
    "# str_sql = \"Select * from company_website_pages Where name = ? and page_url = ?\"\n",
    "# data = [\"Test\",\"www.test.com\"]\n",
    "# rows = dbmgr.select_sql(str_sql,data)\n",
    "# print(rows)\n",
    "\n",
    "str_sql = \"select extracted_data, page_text_words from company_pages where company_name = ? and page_url = ?\"\n",
    "data = [\"francisco partners\",\"https://www.franciscopartners.com/team/jason-brein\"]\n",
    "rows = dbmgr.select_sql(str_sql,data)\n",
    "print(rows[0][0])\n",
    "\n",
    "str_sql = \"Select created_on from company_website_pages Where name = ? and page_url = ?\"\n",
    "data = [\"Test\",\"www.test.com\"]\n",
    "# rows = dbmgr.select_scaler(str_sql,data)\n",
    "# print(rows)\n",
    "\n",
    "str_sql = \"Update company_website_pages Set page_text = ? Where name = ? and page_url = ?\"\n",
    "my_dict = {\"name\":\"Test\",\"page_url\":\"www.test.com\",\"url_type\": \"root\",\"page_text\":\"sample text, not found\",\"is_error\":\"False\" }\n",
    "#data = [\"sample text (updated)\",\"Test\",\"www.test.com\"]\n",
    "# rows = dbmgr.execute_sql(str_sql,data)\n",
    "# print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.sqllite_helper as db_manager\n",
    "import src.settings.constants as const\n",
    "# str_sql = \"Insert into company_master ( name, website,robot_file,team_url) Values(?,?,?,?)\"\n",
    "# db_manager.execute_sql(str_sql,[\"Test\",\"Test\",\"test\",\"Test\"])\n",
    "\n",
    "# openAi_rsp = {}\n",
    "# openAi_rsp[\"my_name\"] = \"kishor\"\n",
    "# str_sql = \"Insert into company (name,page_url,url_type,page_text,page_text_words,team_info_json) Values(?,?,?,?,?,?)\"\n",
    "# db_manager.execute_sql(str_sql,[\"Test\",\"Test_url\",\"team\",\"What is your name?\",4,openAi_rsp])\n",
    "\n",
    "str_sql = \"Select company_website,company_team_url,contact_bio_url, linked_in_profile_url from contact_company where full_name = ?\"\n",
    "db_rsp = db_manager.select_sql(str_sql,[\"Test\"])\n",
    "print(db_rsp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.sqllite_helper as dbmgr\n",
    "import src.util.helper as helper\n",
    "import pandas as pd\n",
    "\n",
    "# #str_sql = \"Select name,page_url,page_text,company_info,team_info, team_info_json,company_info_json from company where STRFTIME('%Y-%m-%d', created_on)  = '2024-08-24'\"\n",
    "# str_sql = \"Select name,page_url,page_text,company_info,team_info, team_info_json,company_info_json from company\"\n",
    "# rows = dbmgr.select_sql(str_sql,[])\n",
    "# df_data = pd.DataFrame(rows, columns=['company_name','page_url','page_text','company_info','team_info','team_info_json','company_info_json'])\n",
    "# df_data.to_excel(\"./output/company_pages.xlsx\",index=False)\n",
    "\n",
    "# #str_sql = \"SELECT id,name,website,robot_file,is_scraping_allowed from company_master where STRFTIME('%Y-%m-%d', created_on)  = '2024-08-24'\"\n",
    "# str_sql = \"SELECT id,name,website,robot_file,is_scraping_allowed from company_master\"\n",
    "# rows = dbmgr.select_sql(str_sql,[])\n",
    "# df_data = pd.DataFrame(rows, columns=['Id','company_name','website','robot_file','is_scraping_allowed'])\n",
    "# df_data.to_excel(\"./output/company_master.xlsx\",index=False)\n",
    "\n",
    "str_sql = \"select id, full_name, linked_in_profile_url from contact_company where linked_in_profile_url != 'Not found'\"\n",
    "rows = dbmgr.select_sql(str_sql,[])\n",
    "df_data = pd.DataFrame(rows, columns=['Id','full_name','linked_in_profile_url'])\n",
    "df_data.to_excel(\"./output/contacts_with_linkedin_profile.xlsx\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.duplicate_finder.DuplicateFinder as dupFind\n",
    "\n",
    "rules = {}\n",
    "# rules[\"Rule1\"] = [\"adam tindall\",\"adam tindel\"]\n",
    "# rules[\"Rule2\"] = [\"adam tindall\",\"tindel\"]\n",
    "# rules[\"Rule3\"] = [\"adam.tindall\",\"adam tindel\"]\n",
    "\n",
    "# rules[\"Rule1\"] = [\"adam freda\",\"adam friedman\"]\n",
    "# rules[\"Rule2\"] = [\"adam freda\",\"afriedman\"]\n",
    "# rules[\"Rule3\"] = [\"adam.freda\",\"adam friedman\"]\n",
    "\n",
    "# rules[\"Rule1\"] = [\"jd nelson\",\"joshua\tnelson\"]\n",
    "# rules[\"Rule2\"] = [\"jd nelson\",\"jnelson\"]\n",
    "# rules[\"Rule3\"] = [\"jnelson\",\"joshua nelson\"]\n",
    "\n",
    "# rules[\"Rule1\"] = [\"doug smith\",\"douglas smith\"]\n",
    "# rules[\"Rule2\"] = [\"doug\tsmith\",\"dsmith\"]\n",
    "# rules[\"Rule3\"] = [\"djsmith\",\"douglas smith\"]\n",
    "\n",
    "rules[\"Rule1\"] = [\"ross\",\"rost\"]\n",
    "# rules[\"Rule2\"] = [\"doug\tsmith\",\"dsmith\"]\n",
    "# rules[\"Rule3\"] = [\"djsmith\",\"douglas smith\"]\n",
    "\n",
    "\n",
    "\n",
    "for key in rules.keys():\n",
    "  r1 = dupFind.get_fuzzy_similarity(rules[key][0],rules[key][1])\n",
    "  r2 = dupFind.get_levenshtein_similarity(rules[key][0],rules[key][1])\n",
    "  r3 = dupFind.get_jaro_winkler_similarity(rules[key][0],rules[key][1])\n",
    "  print(key,\" : \",r1,r2,r3,\" Avg : \",(r1+r2+r3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# df_dup = pd.read_excel(\"./output/TV_Contact_TestData_v1.0_dup.xlsx\")\n",
    "# #print(f\"Memory usage Df Dup before deletion: {df_dup.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# df_flt = df_dup[df_dup[\"dup_group_id\"] != 999999] \n",
    "# #print(f\"Memory usage Df Flt before deletion: {df_flt.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# df_grp =  df_flt.groupby([\"dup_group_id\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "# # #print(f\"Memory usage Df Grp before deletion: {df_grp.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "# # max_dup_group_id = df_grp[\"dup_group_id\"].max()\n",
    "# df_grp = df_grp[df_grp[\"count\"] > 2]\n",
    "\n",
    "# list_dup_grp_ids = df_grp[\"dup_group_id\"].to_list()\n",
    "\n",
    "# def add_rows_where_firstname_lastname_match(df_grp_keep,df_grp_del):\n",
    "# \trows = []\n",
    "# \tlist_index = []\n",
    "# \tfor idx in range(len(df_grp_del)):\n",
    "# \t\trow_del = df_grp_del.iloc[idx]\n",
    "# \t\tfor id in range(len(df_grp_keep)):\n",
    "# \t\t\trow_keep = df_grp_keep.iloc[id]\n",
    "# \t\t\tname_del =  str(row_del[\"FirstName\"]).lower().strip() + \" \" + str(row_del[\"LastName\"]).lower().strip()\n",
    "# \t\t\tname_keep = str(row_keep[\"FirstName\"]).lower().strip() + \" \" + str(row_keep[\"LastName\"]).lower().strip()\n",
    "# \t\t\tif name_del == name_keep:\n",
    "# \t\t\t\trows.append(row_del[\"RowNo\"])\n",
    "# \t\t\t\tlist_index.extend(df_grp_del.index[df_grp_del[\"RowNo\"] == row_del[\"RowNo\"]].to_list())\n",
    "# \t\t\t\tbreak\n",
    "# \t# Now append Rows into df_grp_keep and remove rows from df_grp_del\n",
    "# \tfor row in rows:\n",
    "# \t\tdf_grp_keep =  pd.concat([df_grp_keep, df_grp_del[df_grp_del[\"RowNo\"] == row]], ignore_index=True)\n",
    "# \tdf_grp_del = df_grp_del.drop(list_index, axis=0)\n",
    "\t\t\n",
    "# \treturn df_grp_keep, df_grp_del\t\n",
    "\t\t\n",
    "\n",
    "# # Now Loop through all Duplicate\n",
    "# final_df = pd.DataFrame()\n",
    "# for idx in range(len(df_grp)):\n",
    "# \trow = df_grp.iloc[idx]\n",
    "# \tdf_flt = df_dup[df_dup[\"dup_group_id\"] == row[\"dup_group_id\"]]\n",
    "# \tdf_grp_keep = df_flt.groupby(['Email']).filter(lambda x: len(x) > 1)\n",
    "# \tdf_grp_del = df_flt.groupby(['Email']).filter(lambda x: len(x) == 1)\n",
    "# \tdf_grp_keep, df_grp_del = add_rows_where_firstname_lastname_match(df_grp_keep,df_grp_del)\n",
    "# \tfinal_df = pd.concat([final_df, df_grp_keep[df_grp_del[\"RowNo\"] == row]], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Final Datasheet for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: helper\u001b[38;5;241m.\u001b[39mclean_contact_data(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#df = df.sort_values(by=[\"FirstName\",\"LastName\"])\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFirstName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\t\t\n\u001b[0;32m     10\u001b[0m list_rsp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32mc:\\Users\\thomas.patole.genexa\\Project\\Worker_Duplicat_File\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:7200\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   7198\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 7200\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\n\u001b[0;32m   7202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   7204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32mc:\\Users\\thomas.patole.genexa\\Project\\Worker_Duplicat_File\\.venv\\Lib\\site-packages\\pandas\\core\\sorting.py:439\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    437\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    438\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 439\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    441\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import src.util.helper as helper\n",
    "\n",
    "file_path = \"./data/Comparison of Contact and Company data.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "df = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "df = df.sort_values(by=[\"FirstName\",\"LastName\"])\n",
    "\t\t\n",
    "\n",
    "list_rsp = []\n",
    "for idx, row in df.iterrows():\n",
    "  rsp = {}\n",
    "  df_tmp = df[(df[\"sw_FirstName\"] == row[\"FirstName\"])] #& (df[\"sw_LastName\"] == row[\"LastName\"])] #& (df[\"sw_Email\"] == row[\"Email\"])]\n",
    "  if len(df_tmp) == 0:\n",
    "    #print(row[\"FirstName\"],row[\"LastName\"])\n",
    "    rsp[\"sw_FirstName\"] = \"\"\n",
    "    #rsp[\"sw_LastName\"] = \"\"\n",
    "    #rsp[\"sw_Email\"] = \"\"\n",
    "    #rsp[\"dup_group_id\"] = row[\"dup_group_id\"]\n",
    "    #rsp[\"error_rate\"] = row[\"error_rate\"]\n",
    "    rsp[\"FirstName\"] = row[\"FirstName\"]\n",
    "    #rsp[\"LastName\"] = row[\"LastName\"]\n",
    "    #rsp[\"Email\"] = row[\"Email\"]\n",
    "  else:\n",
    "    rsp[\"sw_FirstName\"] = row[\"FirstName\"]\n",
    "    #rsp[\"sw_LastName\"] = row[\"LastName\"]\n",
    "    #rsp[\"sw_Email\"] = df_tmp[\"sw_Email\"].values[0]\n",
    "    #rsp[\"dup_group_id\"] = row[\"dup_group_id\"]\n",
    "    #rsp[\"error_rate\"] = row[\"error_rate\"]\n",
    "    rsp[\"FirstName\"] = row[\"FirstName\"]\n",
    "    #rsp[\"LastName\"] = row[\"LastName\"]\n",
    "    #rsp[\"Email\"] = row[\"Email\"]\n",
    "  list_rsp.append(rsp)\n",
    "\n",
    "df = df.sort_values(by=[\"sw_FirstName\",\"sw_LastName\"])\t\t\n",
    "for idx, row in df.iterrows():\n",
    "  df_tmp = df[(df[\"FirstName\"] == row[\"sw_FirstName\"])] # & (df[\"LastName\"] == row[\"sw_LastName\"]) ]#& (df[\"Email\"] == row[\"sw_Email\"])]\n",
    "  if len(df_tmp) == 0:\n",
    "    rsp = {} \n",
    "    rsp[\"sw_FirstName\"] = row[\"sw_FirstName\"]\n",
    "    #rsp[\"sw_LastName\"] = row[\"sw_LastName\"]\n",
    "    #rsp[\"sw_Email\"] = row[\"sw_Email\"]\n",
    "    #rsp[\"dup_group_id\"] = 0\n",
    "    #rsp[\"error_rate\"] = 0.0\n",
    "    rsp[\"FirstName\"] = \"\"\n",
    "    #rsp[\"LastName\"] = \"\" \n",
    "    #rsp[\"Email\"] = \"\"\n",
    "    list_rsp.append(rsp)\n",
    "\n",
    "df_final = pd.DataFrame.from_dict(list_rsp)\n",
    "df_final.to_excel(\"./output/Comparison of Contact and Company data_output.xlsx\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Records from Group where Last2Char not matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.util.helper as helper\n",
    "\n",
    "file_path = \"./data/Southfield_BDS_ICRM_ComparisionV3_11.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "df = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "df[\"LastName_2char\"] = df[\"LastName\"].apply(lambda x: x[:2] if isinstance(x, str) else '')\n",
    "df = df.sort_values(by=[\"dup_group_id\",\"LastName\",\"LastName_2char\"])\n",
    "\n",
    "df_grp1 = df.groupby([\"dup_group_id\",\"LastName_2char\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "df_grp1 = df_grp1.sort_values(by=[\"dup_group_id\"])\n",
    "\n",
    "df_grp2 = df.groupby([\"dup_group_id\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "df_grp2 = df_grp2.sort_values(by=[\"dup_group_id\"])\n",
    "\n",
    "\n",
    "df_grp = pd.merge(df_grp2,df_grp1,on=[\"dup_group_id\"])\n",
    "df_flt = df_grp[(df_grp[\"count_x\"] != df_grp[\"count_y\"]) & (df_grp[\"count_y\"] == 1) ]\n",
    "\n",
    "list_IsDel = []\n",
    "for idx in range(len(df)):\n",
    "  row = df.iloc[idx]\n",
    "  df_tmp = df_flt[(df_flt[\"dup_group_id\"] == row[\"dup_group_id\"]) & (df_flt[\"LastName_2char\"] == row[\"LastName_2char\"])]\n",
    "  if len(df_tmp) == 0:\n",
    "    list_IsDel.append(False)\n",
    "  else:\n",
    "    list_IsDel.append(True)  \n",
    "\n",
    "df[\"Is_Del_Row\"] = list_IsDel\n",
    "df.to_excel(\"./output/Last2CharRowDel.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def validate_website_url(url_found:str):\n",
    "  web_list = []\n",
    "  cmp_domain = \"\"\n",
    "  for el in url_found.split(\"/\"):\n",
    "    el = str(el).strip().lower()  \n",
    "    if  el.endswith(\".com\") or el.endswith(\".org\") or el.endswith(\".gov\") or el.endswith(\".info\") or el.endswith(\"edu\") \\\n",
    "        or el.endswith(\".gov.uk\") or el.endswith(\".si\") or el.endswith(\".us\") or el.endswith(\".hk\") or el.endswith(\".io\") \\\n",
    "        or el.endswith(\".net\")  :\n",
    "      cmp_domain = el\n",
    "      web_list.append(el)\n",
    "      break\n",
    "    else:\n",
    "      web_list.append(el)    \n",
    "\n",
    "  # Company domain name should not start with this.\n",
    "  invalid_domains = [\".gov\",\".edu\",\".gov.uk\",\".info\",\".ac.uk\",\".us\",\"en.wikipedia\",\"zoominfo.com\",\"files\",\"brokercheck.finra.org\",\"pitchbook.com\",\"iuk.ktn-uk.org\" \\\n",
    "                    ,\"bloomberg.com\",\"linkedin.com\" ]\n",
    "  is_cmp_domain_contains_invalid_domain = False\n",
    "  for dmn in invalid_domains:\n",
    "    if re.search(dmn,cmp_domain,re.IGNORECASE):\n",
    "      is_cmp_domain_contains_invalid_domain = True\n",
    "      break\n",
    "\n",
    "  #print(\"Company Domain : \",cmp_domain,\", is_cmp_domain_contains_invalid_domain : \", is_cmp_domain_contains_invalid_domain, \", Org Url : \",url_found  )    \n",
    "  if is_cmp_domain_contains_invalid_domain :\n",
    "    return \"Not found\"    \n",
    "  else:\n",
    "    return \"/\".join(web_list) \n",
    "\n",
    "def is_valid_website(el):\n",
    "  if  el.endswith(\".com\") or el.endswith(\".org\") or el.endswith(\".gov\") or el.endswith(\".info\") or el.endswith(\"edu\") \\\n",
    "        or el.endswith(\".gov.uk\") or el.endswith(\".si\"):\n",
    "    rsp = True\n",
    "  else:\n",
    "    rsp = False\n",
    "  return rsp      \n",
    "\n",
    "\n",
    "file_path = \"./output/company_robots_stats.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "\n",
    "new_url = []\n",
    "should_delete = []\n",
    "for idx in range(len(df)):\n",
    "  row = df.iloc[idx]\n",
    "  rsp = validate_website_url(row[\"website\"])\n",
    "  new_url.append(rsp)\n",
    "  \n",
    "  if is_valid_website(row[\"website\"]):\n",
    "    should_delete.append(\"False\")\n",
    "  else:\n",
    "    should_delete.append(\"True\")  \n",
    "\n",
    "df[\"new_url\"] = new_url\n",
    "df[\"should_delete\"] = should_delete\n",
    "df.to_excel(\"./output/company_robots_stat_new.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Text By Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "\"                                         About   Investment Focus  Graham at a Glance  History    Approach   Investment Criteria  Industry Expertise  Environmental, Social, & Governance    Team   Investment Team  Operations Team  Technology & Innovation Council  Accounting & Administration  Graham Internal Advisory  Support Staff    Portfolio   All  Current  Divested    Resources   News & Insights  Downloads  ESG  Culture & Careers    Contact Us  Partner Login       Partner Login                  Close           About   Investment Focus  Graham at a Glance  History    Approach   Investment Criteria  Industry Expertise  Environmental, Social, & Governance    Team   Investment Team  Operations Team  Technology & Innovation Council  Accounting & Administration  Graham Internal Advisory  Support Staff    Portfolio   All  Current  Divested    Resources   News & Insights  Downloads  ESG  Culture & Careers    Contact Us  Partner Login    Copyright 2024, Graham Partners.  All Rights Reserved       YouTube Channel     Privacy   |   Terms     Copyright 2024, Graham Partners.  All Rights Reserved                             Innovation and growth are in our DNA.            We strive to invest in market-leading private businesses spurring innovation in industrial technologies and advanced manufacturing, and partner with those companies to accelerate product development and top-line growth.            SCROLL               Future-forward  investment focus.      Since its founding in 1988, Graham Partners has focused on industrial technology and advanced manufacturing businesses with deep engineering resources capable of leveraging advances in materials science and production technologies to maximize growth potential.      We have the flexibility and dedicated experience to complete buyout transactions or growth investments, applying a bespoke value creation approach that best suits the needs of our partners.        Our investment team works hand-in-hand with our team of over 60 operating executives and aims to deliver hands-on resources to portfolio company management teams to enhance product development capabilities and catalyze revenue growth.                 Graham At a Glance           6.2B    Total assets under management*      160+    Acquisitions & investments to date      21    Active investments          *\n",
    "Represents committed capital raised since inception through the Graham Partners funds together with Graham-led co-investments as of March 31, 2024, pro forma for subsequent events, which differs from Graham’s Regulatory Assets Under Management of approximately $3.7 billion as of March 31, 2024.               A history founded  on innovation.   We believe we are distinct from traditional private investment firms, tracing the roots of our family industrial heritage to The Graham Group, built by engineering entrepreneur Donald C. Graham.     The Graham Group  The origins of The Graham Group date back to a design engineering firm established by Donald C. Graham in a central Pennsylvania farmhouse basement in 1960, which ultimately spawned three successful advanced manufacturing businesses. The Graham Group name has been broadened and today is used to refer to an alliance of independent operating businesses, investment firms and philanthropic entities, which all share in the common legacy of entrepreneur Donald C. Graham.  Graham Partners, The Beginning  Prior to the formation of its first fund in 1999, Graham Partners served as the corporate finance arm for The Graham Group businesses.  Graham Partners, Today  Graham Partners is a private investment firm focused on investing in technology-driven companies that are spurring innovation in advanced manufacturing, resulting in product substitutions, raw materials conversions and disruptions to traditional end markets. Based in suburban Philadelphia and investing primarily across North America, the firm has access to extensive operating resources and industrial expertise and is a member of The Graham Group .               Up Next  Explore our investment approach                     3811 West Chester Pike Building 2, Suite 200 Newtown Square, PA 19073    (610) 408–0500    ©2024 Graham Partners. All Rights Reserved.      Helpful Links  Contact Us  Partner Login  Privacy Policy  Terms of Use  Sitemap                                                 \"\n",
    "\"\"\"\n",
    "\n",
    "import src.duplicate_finder.DuplicateFinder as dup\n",
    "import src.ner.ner_team_company_info as ner\n",
    "ner.validate_sentences_for_designation(input_text)\n",
    "ner.validate_company_info(input_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact Search + Website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.webscrapper.webscrapper as wsc\n",
    "import src.util.helper as helper\n",
    "\n",
    "file_path = \"./data/Southfield_Probable_Dup_contact_200_dup_groups.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "df = df.fillna(\"\")\n",
    "\n",
    "list_1,list_2,list_3,list_4,list_5 = [],[],[],[],[]\n",
    "for idx in range(len(df)):\n",
    "  row = df.iloc[idx]\n",
    "  search_str = row[\"FirstName\"] + \" \" + row[\"LastName\"]\n",
    "  if row[\"Email\"].find(\"@\") > -1:\n",
    "    email_domain = helper.get_domain_from_email(row[\"Email\"])\n",
    "  else:\n",
    "    email_domain = row[\"CompanyName\"]  \n",
    "  rsp = wsc.find_contact_current_company(search_str,email_domain)\n",
    "  list_1.append(rsp[\"is_current_org\"])\n",
    "  list_2.append(rsp[\"source_url\"])\n",
    "  list_3.append(rsp[\"linked_in_profile_url\"])\n",
    "  list_4.append(rsp[\"team_url\"])\n",
    "  list_5.append(rsp[\"website\"])\n",
    "\n",
    "df[\"is_current_org\"] = list_1\n",
    "df[\"source_url\"] = list_2\n",
    "df[\"linked_in_profile_url\"] = list_3\n",
    "df[\"team_url\"] = list_4\n",
    "df[\"website\"] = list_5\n",
    "\n",
    "df.to_excel(\"./output/Southfield_Probable_Dup_contact_200_dup_groups_api_search_29_Aug.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(len(list_1),len(df))\n",
    "# # list_1.append(\"Not found\")\n",
    "# # list_2.append(\"Not found\")\n",
    "# # list_3.append(\"Not found\")\n",
    "# # list_4.append(\"Not found\")\n",
    "# # list_5.append(\"Not found\")\n",
    "# # print(len(list_1),len(df))\n",
    "\n",
    "# df[\"is_current_org\"] = list_1\n",
    "# df[\"source_url\"] = list_2\n",
    "# df[\"linked_in_profile_url\"] = list_3\n",
    "# df[\"team_url\"] = list_4\n",
    "# df[\"website\"] = list_5\n",
    "\n",
    "# df.to_excel(\"./output/Southfield_Probable_Dup_contact_100_dup_groups_api_search.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.webscrapper.webscrapper as wsc\n",
    "page_rsp = wsc.get_page_content(\"https://fgsglobal.com/people/andrew-cole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.helper as helper\n",
    "import src.ner.ner_team_company_info as ner\n",
    "ner_rsp = {}\n",
    "ner_rsp = ner.find_company_and_team_info_from_text_v1(ner_rsp,page_rsp[\"page_text\"],)\n",
    "helper.wrap_and_print(page_rsp[\"page_text\"],no_of_char=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.wrap_and_print(page_rsp[\"page_text\"],no_of_char=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.webscrapper.webscrapper as wsc\n",
    "search_str = \"adam stein\"\n",
    "email_domain = \"lightbay.com\"\n",
    "#tmp_rsp = wsc.find_contact_current_company(search_str, email_domain,\"wafra partners\")\n",
    "tmp_rsp = wsc.get_google_api_resposne_search_items(search_str + \" \" + email_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#print(tmp_rsp[\"data\"])\n",
    "\n",
    "for item in tmp_rsp[\"data\"]:\n",
    "  search_text = str(item[\"title\"]).strip().lower()\n",
    "  # if \"snippet\" in item:\n",
    "  #   search_text += \" \" + str(item[\"snippet\"]).strip().lower()\n",
    "  print(search_text)\n",
    "  print(str(item[\"snippet\"]).strip().lower())\n",
    "  print(item[\"link\"])\n",
    "  # if search_text.find(\"wafra\") > -1:\n",
    "  #   print(search_text)\n",
    "  #   print(item[\"link\"])\n",
    "    \n",
    "#   #print(item[\"pagemap\"][\"metatags\"][0][\"og:description\"])\n",
    "#   # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.util.helper as helper\n",
    "\n",
    "file_path = \"./output/Southfield_Probable_Dup_contact_100_dup_groups_api_search.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "#df.head()\n",
    "df_grp1 =  df.groupby([\"dup_group_id\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "df_grp1 = df_grp1.sort_values(by=[\"dup_group_id\"])\n",
    "df_flt = df[df[\"source_url\"] == \"Not found\"]\n",
    "df_grp2 = df_flt.groupby([\"dup_group_id\",\"source_url\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "df_grp2 = df_grp2.sort_values(by=[\"dup_group_id\"])\n",
    "df_grp = pd.merge(df_grp2,df_grp1,on=[\"dup_group_id\"])\n",
    "df_flt2 = df_grp[df_grp[\"count_x\"] == df_grp[\"count_y\"]]\n",
    "#df_flt2.head(20)\n",
    "print(f\"Total {len(df_flt2)} duplicate groups has all records 'Not Found'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_grp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.proxycurl as pcurl\n",
    "\n",
    "pcurl_rsp = pcurl.get_linkedin_profile(\"https://www.linkedin.com/in/gary-adamson-9b11545fggr/\")\n",
    "\n",
    "print(pcurl_rsp[\"data\"])\n",
    "\n",
    "# import src.webscrapper.webscrapper as wsc\n",
    "# search_str = \"richard beamish\"\n",
    "# email_domain = \"doeren.com\"\n",
    "# company_name = \"doerenmayhew capital advisors\"\n",
    "# contact_rsp = wsc.search_contact_information(search_str,email_domain,company_name)\n",
    "# print(contact_rsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp = {}\n",
    "cnt = 0\n",
    "for exp in pcurl_rsp[\"data\"][\"experiences\"]:\n",
    "\tprint(exp)\n",
    "\tif cnt == 0:\n",
    "\t\trsp[\"llm_curr_org\"]\t= exp[\"company\"]\n",
    "\t\trsp[\"llm_curr_title\"] = exp[\"title\"]\n",
    "\tif cnt == 1:\n",
    "\t\trsp[\"llm_prv_org\"]\t= exp[\"company\"]\n",
    "\t\trsp[\"llm_prv_title\"] = exp[\"title\"]\n",
    "\t\tbreak\n",
    "\tcnt += 1\n",
    "\n",
    "print(rsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Duplicate Flag based on EMail or Prev Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.util.helper as helper\n",
    "\n",
    "file_path = \"./output/TV_Contact_Probable_Duplicate_flag.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "dup_grp_ids = df[\"dup_group_id\"].unique().tolist()\n",
    "\n",
    "for dup_grp_id in dup_grp_ids:\n",
    "\tdf_tmp = df[df[\"dup_group_id\"] == dup_grp_id][[\"RowNo\",\"Email\",\"CompanyName\",\"action\",\"prv_org\"]]\n",
    "\tis_keep_exists = \"Keep\" in df_tmp[\"action\"].values\n",
    "\tif is_keep_exists:\n",
    "\t\tfor id, row in df_tmp.iterrows():\n",
    "\t\t\tflag =\"NO_CHANGE\"\t\n",
    "\t\t\tif row[\"action\"] == \"Not Sure\":\n",
    "\t\t\t\tkeep_email = df_tmp[df_tmp[\"action\"] == \"Keep\"][[\"Email\"]].values[0][0]\n",
    "\t\t\t\tkeep_prv_org = df_tmp[df_tmp[\"action\"] == \"Keep\"][[\"prv_org\"]].values[0][0]\n",
    "\t\t\t\tif str(keep_email).strip() == str(row[\"Email\"]).strip():\n",
    "\t\t\t\t\tflag = \"Duplicate\"\n",
    "\t\t\t\telif ( keep_prv_org != \"Not found\"):\n",
    "\t\t\t\t\tif ( keep_prv_org == str(row[\"prv_org\"]).strip()):\n",
    "\t\t\t\t\t\tflag = \"Duplicate\"\t\n",
    "\t\t\t\tprint(dup_grp_id,row[\"RowNo\"],flag)\n",
    "\n",
    "\ttry:\n",
    "\t\tkeep_count = \tint(df_tmp[\"action\"].value_counts()[\"Keep\"])\n",
    "\texcept Exception as e:\n",
    "\t\tkeep_count = 0\n",
    "\n",
    "\tif len(df_tmp) == keep_count:\n",
    "\t\tcnt = 0\n",
    "\t\tfor id, row in df_tmp.iterrows():\n",
    "\t\t\tflag =\"NO_CHANGE\"\t\n",
    "\t\t\tif cnt == 0:\n",
    "\t\t\t\tkeep_email = df_tmp[df_tmp[\"action\"] == \"Keep\"][[\"Email\"]].values[0][0]\n",
    "\t\t\t\tkeep_prv_org = df_tmp[df_tmp[\"action\"] == \"Keep\"][[\"prv_org\"]].values[0][0]\t\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tif keep_email == str(row[\"Email\"]).strip():\n",
    "\t\t\t\t\tflag = \"Duplicate\"\n",
    "\t\t\t\telif ( keep_prv_org != \"Not found\"):\n",
    "\t\t\t\t\tif ( keep_prv_org == str(row[\"prv_org\"]).strip()):\n",
    "\t\t\t\t\t\tflag = \"Duplicate\"\t\n",
    "\t\t\t\tprint(dup_grp_id,row[\"RowNo\"],flag)\n",
    "\t\t\tcnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy Curl Response Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.util.proxycurl as pc_helper\n",
    "\n",
    "file_path = \"./data/contacts_with_linkedin_profile_50.xlsx\"\n",
    "df = pd.read_excel(file_path) \n",
    "p_rsp = []\n",
    "for id, row in df.iterrows():\n",
    "  rsp = pc_helper.get_linkedin_profile(row[\"linked_in_profile_url\"])\n",
    "  p_rsp.append(str(rsp))\n",
    "df[\"proxy_curl_response\"] = p_rsp\n",
    "\n",
    "df.to_excel(\"./output/contacts_with_linkedin_profile_50_with_response.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Keep/Duplicate Flag Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.settings.constants as const\n",
    "import src.util.proxycurl as pc_helper\n",
    "\n",
    "file_path = \"./output/crm_contacts_with_all_scenario_with_flag.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "df = df.fillna(\"\")\n",
    "df.head()\n",
    "# First Validation\n",
    "df_tmp = df[df[\"action\"] == \"Not Sure\"]\n",
    "group_ids = df_tmp[\"dup_group_id\"].unique().tolist()\n",
    "if len(df_tmp) != 0:\n",
    "  print(f\"These duplicate group {group_ids} has not been resolved yet. Please mark all groups with keep/duplicate falg.\")\n",
    "\n",
    "# #Rule 2\n",
    "# df_grp1 = df.groupby([\"dup_group_id\",\"action\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "# df_grp1 = df_grp1.sort_values(by=[\"dup_group_id\"])\n",
    "# df_grp2 = df.groupby([\"dup_group_id\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "# df_grp2 = df_grp2.sort_values(by=[\"dup_group_id\"])\n",
    "# df_grp = pd.merge(df_grp2,df_grp1,on=[\"dup_group_id\"])\n",
    "# df_flt = df_grp[(df_grp[\"count_x\"] == df_grp[\"count_y\"]) & (df_grp[\"count_y\"] == 1) ]\n",
    "# df_flt.head(50)\n",
    "\n",
    "df_flt = df[df[\"dup_group_id\"] != 999999]\n",
    "\n",
    "df_flt.head()\n",
    "dup_grp_ids = df_flt[\"dup_group_id\"].unique().tolist()\n",
    "\n",
    "list_rsp = []\n",
    "rsp = {}\n",
    "for grp_id in dup_grp_ids:\n",
    "\torg_count,email_count,phone_count,desg_count=0,0,0,0\n",
    "\tdf_tmp1 = df_flt[(df_flt[\"dup_group_id\"] == grp_id) & (df_flt[\"action\"] == \"Keep\") ]\n",
    "\tkeep_row = df_tmp1.iloc[0]\n",
    "\tif keep_row[\"source\"] == \"Exact Duplicate\":\n",
    "\t\tcontinue\n",
    "\tif keep_row[\"prv_org\"] != const.NOT_FOUND:\n",
    "\t\tif str(keep_row[\"CompanyName\"]).strip() != str(keep_row[\"prv_org\"]).strip():\n",
    "\t\t\torg_count +=1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"CompanyName\"+str(org_count)\n",
    "\t\t\trsp[\"col_val\"] = str(keep_row[\"prv_org\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\tif keep_row[\"prv_title\"] != const.NOT_FOUND:\n",
    "\t\tif str(keep_row[\"Designation\"]).strip() != str(keep_row[\"prv_title\"]).strip():\n",
    "\t\t\tdesg_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Designation\" + str(desg_count)\n",
    "\t\t\trsp[\"col_val\"] = str(keep_row[\"prv_title\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\tif keep_row[\"new_email\"] != const.NOT_FOUND:\n",
    "\t\tif str(keep_row[\"Email\"]).strip() != str(keep_row[\"new_email\"]).strip():\n",
    "\t\t\temail_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Email\" + str(email_count)\n",
    "\t\t\trsp[\"col_val\"] = str(keep_row[\"new_email\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t# if keep_row[\"new_phone\"] != const.NOT_FOUND:\n",
    "\t# \tif str(keep_row[\"Phone\"]).strip() != str(keep_row[\"new_phone\"]).strip():\n",
    "\t# \t\tphone_count += 1\n",
    "\t# \t\trsp = {}\n",
    "\t# \t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t# \t\trsp[\"col_name\"] = \"Phone\" + str(phone_count)\n",
    "\t# \t\trsp[\"col_val\"] = str(keep_row[\"new_phone\"]).strip() \n",
    "\t# \t\tlist_rsp.append(rsp)\n",
    "\n",
    "\tdf_tmp2 = df_flt[(df_flt[\"dup_group_id\"] == grp_id) & (df_flt[\"action\"] != \"Keep\") ]\t\n",
    "\tfor id, row in df_tmp2.iterrows():\n",
    "\t\tif str(row[\"Email\"]).strip() != str(keep_row[\"Email\"]).strip():\n",
    "\t\t\temail_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Email\" + str(email_count)\n",
    "\t\t\trsp[\"col_val\"] = str(row[\"Email\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t\tif str(row[\"Designation\"]).strip() != str(keep_row[\"Designation\"]).strip():\n",
    "\t\t\tdesg_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Designation\" + str(desg_count)\n",
    "\t\t\trsp[\"col_val\"] = str(row[\"Designation\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t\tif str(row[\"CompanyName\"]).strip() != str(keep_row[\"CompanyName\"]).strip():\n",
    "\t\t\torg_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"CompanyName\"+str(org_count)\n",
    "\t\t\trsp[\"col_val\"] = str(row[\"CompanyName\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t\t# if str(row[\"Phone\"]).strip() != str(keep_row[\"Phone\"]).strip():\n",
    "\t\t# \tphone_count += 1\n",
    "\t\t# \trsp = {}\n",
    "\t\t# \trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t# \trsp[\"col_name\"] = \"Phone\"\n",
    "\t\t# \trsp[\"col_val\"] = str(row[\"CompanyName\"]).strip() \n",
    "\t\t# \tlist_rsp.append(rsp)\n",
    "\n",
    "for rsp in list_rsp:\n",
    "\tif rsp[\"col_name\"] not in df.columns:\n",
    "\t\tdf[rsp[\"col_name\"]] = \"\"\n",
    "\tdf.loc[df[\"RowNo\"] == rsp[\"RowNo\"], [ rsp[\"col_name\"]]]\t= [rsp[\"col_val\"]]\n",
    "\n",
    "df.to_excel(\"./output/contacts_with_linkedin_profile_50_with_response.xlsx\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Validation\n",
    "df_tmp = df[df[\"action\"] == \"Not Sure\"]\n",
    "group_ids = df_tmp[\"dup_group_id\"].unique().tolist()\n",
    "if len(df_tmp) != 0:\n",
    "  print(f\"These duplicate group {group_ids} has not been resolved yet. Please mark all groups with keep/duplicate falg.\")\n",
    "\n",
    "# #Rule 2\n",
    "# df_grp1 = df.groupby([\"dup_group_id\",\"action\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "# df_grp1 = df_grp1.sort_values(by=[\"dup_group_id\"])\n",
    "# df_grp2 = df.groupby([\"dup_group_id\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "# df_grp2 = df_grp2.sort_values(by=[\"dup_group_id\"])\n",
    "# df_grp = pd.merge(df_grp2,df_grp1,on=[\"dup_group_id\"])\n",
    "# df_flt = df_grp[(df_grp[\"count_x\"] == df_grp[\"count_y\"]) & (df_grp[\"count_y\"] == 1) ]\n",
    "# df_flt.head(50)\n",
    "\n",
    "df_flt = df[df[\"dup_group_id\"] != 999999]\n",
    "\n",
    "df_flt.head()\n",
    "dup_grp_ids = df_flt[\"dup_group_id\"].unique().tolist()\n",
    "\n",
    "list_rsp = []\n",
    "rsp = {}\n",
    "for grp_id in dup_grp_ids:\n",
    "\torg_count,email_count,phone_count,desg_count=0,0,0,0\n",
    "\tdf_tmp1 = df_flt[(df_flt[\"dup_group_id\"] == grp_id) & (df_flt[\"action\"] == \"Keep\") ]\n",
    "\tkeep_row = df_tmp1.iloc[0]\n",
    "\tif keep_row[\"source\"] == \"Exact Duplicate\":\n",
    "\t\tcontinue\n",
    "\tif keep_row[\"prv_org\"] != const.NOT_FOUND:\n",
    "\t\tif str(keep_row[\"CompanyName\"]).strip() != str(keep_row[\"prv_org\"]).strip():\n",
    "\t\t\torg_count +=1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"CompanyName\"+str(org_count)\n",
    "\t\t\trsp[\"col_val\"] = str(keep_row[\"prv_org\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\tif keep_row[\"prv_title\"] != const.NOT_FOUND:\n",
    "\t\tif str(keep_row[\"Designation\"]).strip() != str(keep_row[\"prv_title\"]).strip():\n",
    "\t\t\tdesg_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Designation\" + str(desg_count)\n",
    "\t\t\trsp[\"col_val\"] = str(keep_row[\"prv_title\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\tif keep_row[\"new_email\"] != const.NOT_FOUND:\n",
    "\t\tif str(keep_row[\"Email\"]).strip() != str(keep_row[\"new_email\"]).strip():\n",
    "\t\t\temail_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Email\" + str(email_count)\n",
    "\t\t\trsp[\"col_val\"] = str(keep_row[\"new_email\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t# if keep_row[\"new_phone\"] != const.NOT_FOUND:\n",
    "\t# \tif str(keep_row[\"Phone\"]).strip() != str(keep_row[\"new_phone\"]).strip():\n",
    "\t# \t\tphone_count += 1\n",
    "\t# \t\trsp = {}\n",
    "\t# \t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t# \t\trsp[\"col_name\"] = \"Phone\" + str(phone_count)\n",
    "\t# \t\trsp[\"col_val\"] = str(keep_row[\"new_phone\"]).strip() \n",
    "\t# \t\tlist_rsp.append(rsp)\n",
    "\n",
    "\tdf_tmp2 = df_flt[(df_flt[\"dup_group_id\"] == grp_id) & (df_flt[\"action\"] != \"Keep\") ]\t\n",
    "\tfor id, row in df_tmp2.iterrows():\n",
    "\t\tif str(row[\"Email\"]).strip() != str(keep_row[\"Email\"]).strip():\n",
    "\t\t\temail_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Email\" + str(email_count)\n",
    "\t\t\trsp[\"col_val\"] = str(row[\"Email\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t\tif str(row[\"Designation\"]).strip() != str(keep_row[\"Designation\"]).strip():\n",
    "\t\t\tdesg_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"Designation\" + str(desg_count)\n",
    "\t\t\trsp[\"col_val\"] = str(row[\"Designation\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t\tif str(row[\"CompanyName\"]).strip() != str(keep_row[\"CompanyName\"]).strip():\n",
    "\t\t\torg_count += 1\n",
    "\t\t\trsp = {}\n",
    "\t\t\trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t\trsp[\"col_name\"] = \"CompanyName\"+str(org_count)\n",
    "\t\t\trsp[\"col_val\"] = str(row[\"CompanyName\"]).strip() \n",
    "\t\t\tlist_rsp.append(rsp)\n",
    "\t\t# if str(row[\"Phone\"]).strip() != str(keep_row[\"Phone\"]).strip():\n",
    "\t\t# \tphone_count += 1\n",
    "\t\t# \trsp = {}\n",
    "\t\t# \trsp[\"RowNo\"] = keep_row[\"RowNo\"]\n",
    "\t\t# \trsp[\"col_name\"] = \"Phone\"\n",
    "\t\t# \trsp[\"col_val\"] = str(row[\"CompanyName\"]).strip() \n",
    "\t\t# \tlist_rsp.append(rsp)\n",
    "\n",
    "for rsp in list_rsp:\n",
    "\tif rsp[\"col_name\"] not in df.columns:\n",
    "\t\tdf[rsp[\"col_name\"]] = \"\"\n",
    "\tdf.loc[df[\"RowNo\"] == rsp[\"RowNo\"], [ rsp[\"col_name\"]]]\t= [rsp[\"col_val\"]]\n",
    "\n",
    "df_flt = df[df[\"dup_group_id\"] != 999999]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
