{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact De Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.helper as helper\n",
    "import src.settings.constants as const\n",
    "import src.webscrapper.webscrapper as wsc\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-Sep-26 16:31:30 - WebScrapper - INFO - For company bitnomial found website https://bitnomial.com.\n",
      "2024-Sep-26 16:31:31 - WebScrapper - INFO - For company blackedge capital found website https://www.blackedge.com.\n",
      "2024-Sep-26 16:31:33 - WebScrapper - INFO - For company blue trading found website https://www.bluetradingsystems.com.\n",
      "2024-Sep-26 16:31:34 - WebScrapper - INFO - For company candor found website https://www.candorcs.org.\n",
      "2024-Sep-26 16:31:35 - WebScrapper - INFO - For company capfund group found website https://www.capfundinc.com.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://bitnomial.com</td>\n",
       "      <td>Bitnomial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.blackedge.com</td>\n",
       "      <td>BlackEdge Capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.bluetradingsystems.com</td>\n",
       "      <td>Blue Trading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.candorcs.org</td>\n",
       "      <td>Candor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.capfundinc.com</td>\n",
       "      <td>CapFund Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              website            company\n",
       "0               https://bitnomial.com          Bitnomial\n",
       "1           https://www.blackedge.com  BlackEdge Capital\n",
       "2  https://www.bluetradingsystems.com       Blue Trading\n",
       "3            https://www.candorcs.org             Candor\n",
       "4          https://www.capfundinc.com      CapFund Group"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list = [\"Bitnomial\",\n",
    "\"BlackEdge Capital\",\n",
    "\"Blue Trading\",\n",
    "\"Candor\",\n",
    "\"CapFund Group\"]\n",
    "\n",
    "list_rsp = []\n",
    "for url in url_list:\n",
    "\trsp = {}\n",
    "\twebsite = wsc.get_company_website(url)\n",
    "\trsp[\"website\"] = website\n",
    "\trsp[\"company\"] = url\n",
    "\tlist_rsp.append(rsp)\n",
    "\n",
    "df = pd.DataFrame.from_dict(list_rsp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp1 = wsc.get_company_website_and_legal(\"A.S.R.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legal_name(text:str):\n",
    "\tpattern = r'\\b[A-Za-z0-9\\s,]+?\\s(Inc\\.|LLC|Ltd\\.|Corp\\.|Corporation|Co\\.|Limited|LLP|L\\.P\\.|PLC|GmbH|S\\.A\\.|Pty\\. Ltd\\.|BV)\\b'\n",
    "\tmatch = re.search(pattern, text)\n",
    "\tif match:\n",
    "\t\t\treturn match.group(0)\n",
    "\telse:\n",
    "\t\t\treturn None\n",
    "text = \"ASR Group International, Inc. is the worldâ€™s largest refiner and marketer of cane sugar with an annual production capacity of 6 million metric tons of sugar. We produce a full line of grocery, industrial, foodservice and specialty sweetener products, with a strong focus on innovation and product development. We hold leading positions in our core markets, which include some of the largest end-markets for sweeteners in the world.\"\n",
    "print(extract_legal_name(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "def is_scraping_allowed_by_robots_file(url, robots_text, user_agent='*'):\n",
    "\t# Initialize and parse the robots.txt\n",
    "\trp = RobotFileParser()\n",
    "\t\n",
    "\tprint(rp.parse(robots_text.splitlines()))\n",
    "\t# Check if the website allows scraping for the given user agent\n",
    "\treturn rp.can_fetch(user_agent, url)\n",
    "\n",
    "is_scraping_allowed_by_robots_file(url='https://seasideequity.com/team/andrew-thompson/',robots_text='seasideequity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.log_helper as log_helper\n",
    "import src.settings.constants as const\n",
    "import src.util.helper as helper\n",
    "import src.util.sqllite_helper as db_manager\n",
    "import src.util.api_helper as api_helper\n",
    "import src.util.proxycurl as pcurl\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from rapidfuzz import fuzz\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from src.util.similarity_helper import get_fuzzy_similarity\n",
    "\n",
    "def get_website_robot_file(website_url : str, company_name :str):\n",
    "\t\n",
    "\trobot_file = \"\"\n",
    "\tstr_sql = \"Select robot_file  from company_master Where website = ? COLLATE NOCASE\"\n",
    "\trobot_file = db_manager.select_scaler(str_sql,[website_url])\n",
    "\tif robot_file != const.NOT_EXISTS:\n",
    "\t\t#logger.info(f'company {company_name} and robot file exists in db.')\n",
    "\t\treturn robot_file\n",
    "\n",
    "\t# As Robot file does not exists in DB, Download from Website and Insert into DB\n",
    "\ttry:\n",
    "\t\twith requests.get(website_url + \"/robots.txt\") as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\t\t\tfile_content =  io.StringIO(r.text)\n",
    "\t\t\tfor line in file_content:\n",
    "\t\t\t\tif robot_file == const.NOT_EXISTS:\n",
    "\t\t\t\t\trobot_file = line\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\trobot_file += \"\\n\" + line\t\n",
    "\texcept Exception as e:\n",
    "\t\t#logger.exception(e)\n",
    "\t\trobot_file = const.NOT_FOUND\n",
    "\n",
    "\t# Now Insert Robot file and other content into DB\n",
    "\tstr_sql = \"Insert into company_master ( company_name, website,robot_file) Values(?,?,?)\"\n",
    "\tdb_manager.execute_sql(str_sql,[company_name,website_url,robot_file])\n",
    "\n",
    "\treturn robot_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "robat_files = get_website_robot_file(website_url='https://www.maximgrp.com/about\t',company_name='seasideequity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util.log_helper as log_helper\n",
    "import src.settings.constants as const\n",
    "import src.util.helper as helper\n",
    "import src.util.sqllite_helper as db_manager\n",
    "import src.util.api_helper as api_helper\n",
    "import src.util.proxycurl as pcurl\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from rapidfuzz import fuzz\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from src.util.similarity_helper import get_fuzzy_similarity\n",
    "import src.webscrapper.webscrapper as web\n",
    "def get_page_content(company_name:str,page_url:str, robot_file ):\n",
    "\t\"\"\"\n",
    "\tThis Function will go through Website specific page Find page text content.\n",
    "\t\"\"\"\n",
    "\trsp = {}\n",
    "\trsp[\"is_success\"] = False\n",
    "\n",
    "\tstr_sql = \"SELECT page_text from company_pages where page_url= ? and company_name=?\"\n",
    "\tres = db_manager.select_scaler(str_sql,[page_url,company_name])\n",
    "\tif res != const.NOT_EXISTS:\n",
    "\t\trsp[\"page_text\"]= res\n",
    "\t\trsp[\"is_success\"] = True\n",
    "\t\treturn rsp\n",
    "\n",
    "\tif robot_file == const.NOT_FOUND:\n",
    "\t\tis_url_allowed_to_scrap = True\n",
    "\telse:\n",
    "\t\tis_url_allowed_to_scrap = is_scraping_allowed_by_robots_file(page_url,robot_file)\n",
    "\n",
    "\tif not is_url_allowed_to_scrap:\n",
    "\t\t#logger.info(f\"url {page_url} is not allowed scrap by robot.txt file.\")\n",
    "\t\trsp[\"page_text\"] = \"SCRAPPING_NOT_ALLOWED\"\t\n",
    "\t\treturn rsp\n",
    "\t\n",
    "\t#logger.info(f\"Request started for : {page_url}\")\n",
    "\ttry:\n",
    "\t\theaders = {'User-Agent': random.choice(const.USER_AGENTS)}\n",
    "\t\twith requests.get(page_url,headers=headers, timeout=const.REQUEST_TIME_OUT) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\t\t\trsp[\"page_text\"] = helper.clean_scrapped_text(web.text_from_html(r.text)) \n",
    "\t\t\trsp[\"is_success\"] = True\n",
    "\texcept Exception as e:\n",
    "\t\trsp[\"page_text\"] = \"ERROR\" \n",
    "\t\trsp[\"is_success\"] = False\n",
    "\t\t#logger.error(f\"error occured : {e}\")\n",
    "\t#logger.info(f\"Request Completed for : {page_url}\")\n",
    "\tif rsp[\"is_success\"]:\n",
    "\t\turl_type = web.get_url_type(page_url)\n",
    "\t\tstr_sql=\"INSERT INTO company_pages (company_name, page_url, page_text,url_type) VALUES (?, ?, ?,?)\"\n",
    "\t\tdb_manager.execute_sql(str_sql,[company_name,page_url,rsp[\"page_text\"],url_type])\t\t\n",
    "\treturn rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp = get_page_content(\"maxim group\",\"https://www.maximgrp.com/about\",robat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsp['page_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
