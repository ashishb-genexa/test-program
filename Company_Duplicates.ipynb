{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('code'):\n",
    "    os.makedirs('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - Started processing...\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - reading input file and preparing dataframe.\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - Finding duplicate rows.\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - Assigning group id to duplicate rows.\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - duplicate identification Total Processing Time : 0.12 Seconds\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - Started duplicate resolution.\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - Saving file with duplicate groups and Keep delete flag ./output//Company_Data_with_flag.csv\n",
      "2024-Sep-16 11:07:24 - company_deduplication - INFO - duplicate resolution Total Processing Time : 0.03 Seconds\n"
     ]
    }
   ],
   "source": [
    "import src.company_deduplcation as cmp_dup\n",
    "df = cmp_dup.process_company_duplicator(\"./data/Company_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.settings import constants as const\n",
    "from src.util import helper \n",
    "import pandas as pd\n",
    "\n",
    "def read_file_prep_dataframe(file_path, file_ext):\n",
    "\tif file_ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path,encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path) \n",
    "\n",
    "\tdf = df.fillna(\"\") # Fill Empty value as all columns are strings only\n",
    "\t\t#df.to_excel(\"./output/crm_contact_with_row_no.xlsx\")\n",
    "\n",
    "\t# Clean text of all dataframe text data one time\n",
    "\tdf = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "\t## Add New Columns to Add into original Dataframe\n",
    "\tdf.insert(0,const.COL_DUP_GROUP_ID,999999)\n",
    "\tif const.COL_ID not in df.columns:\n",
    "\t\t# Add RowNum to DataFrame\n",
    "\t\tdf[const.COL_ID] = range(0+1,len(df)+1)\n",
    "\tdf.insert(2,const.COL_ERROR_RATE,100.00)\n",
    "\tdf[const.COL_FUZZ_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_LEVENSHTEIN_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_JARO_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_DUP_ROW_GROUP] = \"\"\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_company_name(company_name):\n",
    "    # Define keywords to be removed\n",
    "    keywords = ['LLP', 'LLC', 'INC', 'PVT', 'LTD', 'CORP', 'CO']\n",
    "    # Create a regex pattern to match any of the keywords\n",
    "    pattern = r'\\b(?:' + '|'.join(keywords) + r')\\b\\.?'\n",
    "    # Substitute the keywords with an empty string\n",
    "    cleaned_name = re.sub(pattern, '', company_name, flags=re.IGNORECASE)\n",
    "    # Remove extra spaces\n",
    "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip()\n",
    "    return cleaned_name\n",
    "\n",
    "# Example usage\n",
    "company_names = [\n",
    "    \"Tech Innovations LLC\",\n",
    "    \"Green Earth Inc.\",\n",
    "    \"Alpha Beta Corp\",\n",
    "    \"Creative Minds LLP\",\n",
    "    \"XYZ Pvt Ltd\"\n",
    "]\n",
    "\n",
    "cleaned_names = [clean_company_name(name) for name in company_names]\n",
    "print(cleaned_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_https(url):\n",
    "    # Check if the URL starts with \"https://\"\n",
    "    if url.startswith(\"https://\"):\n",
    "        # Remove \"https://\" from the URL\n",
    "        return url[8:]\n",
    "    return url\n",
    "\n",
    "# Example usage\n",
    "websites = [\n",
    "    \"https://www.example.com\",\n",
    "    \"https://genexa.ai\",\n",
    "    \"http://www.test.com\",\n",
    "    \"www.nodomain.com\"\n",
    "]\n",
    "cleaned_websites = [remove_https(website) for website in websites]\n",
    "print(cleaned_websites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/Company_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Company Name\"].apply(lambda x: clean_company_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import jellyfish as jfish\n",
    "import Levenshtein\n",
    "def compare_rows(src_row,trg_row):\n",
    "\t\"\"\"\n",
    "\t\tBased On Compare Columns, Calculate Levenstine, Jaro Winkle and Fuzzy raito and select\n",
    "\t\twhere Error rate lowest or similarity ratio is higher\n",
    "\t\"\"\"\n",
    "\tcomparesion_list = []\n",
    "\tid = 0\n",
    "\tmin_error = 100.0\n",
    "\tmin_error_id = -1\n",
    "\tCOMPARE_COLUMNS=[{ \"name\" : \"Rule1\", \"src_cols\":[\"Link\",], \"trg_cols\" : [\"Link\",]},]\n",
    "\tfor comp_columns in COMPARE_COLUMNS:\n",
    "\t\tstr_src = get_col_value(src_row,comp_columns['src_cols'])\n",
    "\t\tstr_trg = get_col_value(trg_row,comp_columns['trg_cols'])\n",
    "\t\tres = {}\n",
    "\t\tres[\"name\"] = comp_columns[\"name\"]\n",
    "\t\tres[\"f\" + str(id)] = get_fuzzy_similarity(str_src,str_trg) \n",
    "\t\tres[\"l\" + str(id)] = get_levenshtein_similarity(str_src,str_trg)\n",
    "\t\tres[\"j\" + str(id)] = get_jaro_winkler_similarity(str_src,str_trg)\n",
    "\t\t#logger.info(f\"src val : {str_src}, trg val : {str_trg}, fuzz : {res['f' + str(id)] }, levst : {res['l' + str(id)]}, jero : {res['j' + str(id)]}\")\n",
    "\t\tres[\"e\" + str(id)] =  (1.0-(float(res[\"f\" + str(id)]) + float(res[\"l\" + str(id)]) + float(res[\"j\" + str(id)]) )/3)*100\n",
    "\t\tif ( float(res[\"e\" + str(id)]) < min_error):\n",
    "\t\t\tmin_error = res[\"e\" + str(id)]\n",
    "\t\t\tmin_error_id = id\n",
    "\t\tcomparesion_list.append(res)\n",
    "\t\tid += 1\t\n",
    "\t#logger.info(f\"min error rate : {min_error}, min error id : {min_error_id}\")\t\n",
    "\treturn min_error_id, comparesion_list[min_error_id]\n",
    "\n",
    "def get_fuzzy_similarity(str_src,str_trg):\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn (fuzz.token_set_ratio(str_src, str_trg)/100)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "def get_levenshtein_similarity(str_src,str_trg):\n",
    "\t\"\"\"\n",
    "\tLevenshtein distance measures the minimum number of single-character edits required to change one string into another.\n",
    "\t\"\"\"\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn Levenshtein.ratio(str_src, str_trg)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "def get_jaro_winkler_similarity(str_src,str_trg):\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn jfish.jaro_winkler_similarity(str_src, str_trg)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "def get_col_value(row : any,col_list : list) -> str:\n",
    "\tval_list = []\n",
    "\tfor col in col_list:\n",
    "\t\tval_list.append(str(row[col]).strip())\n",
    "\treturn \" \".join(val_list)\n",
    "\n",
    "import src.settings.constants as const\n",
    "def find_dup_row_by_sequence(df,sort_col):\n",
    "\tdf = df.sort_values(by=[sort_col])\n",
    "\tlist_dup_rows = []\n",
    "\t# Now Iterate through Each record and try to find out Group\n",
    "\tfor src_idx in range(len(df)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "\t\tfor trg_idx in range(len(df)):\n",
    "\t\t\ttrg_row = df.iloc[trg_idx]\n",
    "\t\t\t#logger.info(f\"source index {src_idx} ,target index : {trg_idx}\")\n",
    "\t\t\tif trg_idx > src_idx:\n",
    "\t\t\t\tdup_grp = {}\n",
    "\t\t\t\tdup_grp[const.COL_SOURCE_ID] = src_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[const.COL_TARGET_ID] = trg_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[\"src_Link\"] = str(src_row[\"Link\"])\n",
    "\t\t\t\tdup_grp[\"trg_Link\"] = str(trg_row[\"Link\"])\n",
    "\t\t\t\tid, rsp = compare_rows(src_row,trg_row)\n",
    "\t\t\t\t#logger.info(id, rsp)\n",
    "\t\t\t\tdup_grp[const.COL_FUZZ_SIMILARITY] = float(rsp[\"f\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_LEVENSHTEIN_SIMILARITY] = float(rsp[\"l\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_JARO_SIMILARITY] = float(rsp[\"j\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_ERROR_RATE] = float(rsp[\"e\" + str(id)])\n",
    "\t\t\t\tif dup_grp[const.COL_ERROR_RATE] <= const.MAX_ERROR_RATE:\n",
    "\t\t\t\t\tlist_dup_rows.append(dup_grp)\n",
    "\treturn list_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dup_row_by_company_website(df,col_first_name=\"Company Name\"):\n",
    "\tlist_dup_rows = []\n",
    "\tdf_dup = df.groupby([\"Link\"], as_index=False).agg(count=(\"Link\", 'count'))\n",
    "\tdf_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "\tfor id, row in df_dup.iterrows():\n",
    "\t\tdf_flt = df[df[\"Link\"] == row[\"Link\"]]\n",
    "\t\tdf_flt = df_flt.sort_values(by=[col_first_name])\n",
    "\t\tlist_dup_rows.extend(find_dup_row_by_sequence(df=df_flt,sort_col=col_first_name))\n",
    "\t\n",
    "\t# # Now Iterate through dup Rows and If last name matching Error rate is higher than threshold,\n",
    "\t# # Remove those rows from List.\n",
    "\t# list_dup_rows = remove_dup_rows_by_lastname_match_similarity(list_dup_rows)\n",
    "\t\n",
    "\tdf_dup_rows = pd.DataFrame.from_dict(list_dup_rows)\n",
    "\treturn df_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.settings.constants as const\n",
    "def find_dup_row_by_sequence(df,sort_col):\n",
    "\tdf = df.sort_values(by=[sort_col])\n",
    "\tlist_dup_rows = []\n",
    "\t# Now Iterate through Each record and try to find out Group\n",
    "\tfor src_idx in range(len(df)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "\t\tfor trg_idx in range(len(df)):\n",
    "\t\t\ttrg_row = df.iloc[trg_idx]\n",
    "\t\t\t#logger.info(f\"source index {src_idx} ,target index : {trg_idx}\")\n",
    "\t\t\tif trg_idx > src_idx:\n",
    "\t\t\t\tdup_grp = {}\n",
    "\t\t\t\tdup_grp[const.COL_SOURCE_ID] = src_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[const.COL_TARGET_ID] = trg_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[\"src_Link\"] = str(src_row[\"Link\"])\n",
    "\t\t\t\tdup_grp[\"trg_Link\"] = str(trg_row[\"Link\"])\n",
    "\t\t\t\tid, rsp = compare_rows(src_row,trg_row)\n",
    "\t\t\t\t#logger.info(id, rsp)\n",
    "\t\t\t\tdup_grp[const.COL_FUZZ_SIMILARITY] = float(rsp[\"f\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_LEVENSHTEIN_SIMILARITY] = float(rsp[\"l\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_JARO_SIMILARITY] = float(rsp[\"j\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_ERROR_RATE] = float(rsp[\"e\" + str(id)])\n",
    "\t\t\t\tif dup_grp[const.COL_ERROR_RATE] <= const.MAX_ERROR_RATE:\n",
    "\t\t\t\t\tlist_dup_rows.append(dup_grp)\n",
    "\treturn list_dup_rows\n",
    "\n",
    "\n",
    "def find_dup_row_by_company_website(df,col_first_name=\"Company Name\"):\n",
    "\tlist_dup_rows = []\n",
    "\tdf_dup = df.groupby([\"Link\"], as_index=False).agg(count=(\"Link\", 'count'))\n",
    "\tdf_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "\tfor id, row in df_dup.iterrows():\n",
    "\t\tdf_flt = df[df[\"Link\"] == row[\"Link\"]]\n",
    "\t\tdf_flt = df_flt.sort_values(by=[col_first_name])\n",
    "\t\tlist_dup_rows.extend(find_dup_row_by_sequence(df=df_flt,sort_col=col_first_name))\n",
    "\t\n",
    "\t# # Now Iterate through dup Rows and If last name matching Error rate is higher than threshold,\n",
    "\t# # Remove those rows from List.\n",
    "\t# list_dup_rows = remove_dup_rows_by_lastname_match_similarity(list_dup_rows)\n",
    "\t\n",
    "\tdf_dup_rows = pd.DataFrame.from_dict(list_dup_rows)\n",
    "\treturn df_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from src.util import helper\n",
    "\n",
    "from src.settings import constants as const\n",
    "from src.util import helper \n",
    "import pandas as pd\n",
    "\n",
    "def read_file_prep_dataframe(file_path, file_ext):\n",
    "\tif file_ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path,encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path) \n",
    "\n",
    "\tdf = df.fillna(\"\") # Fill Empty value as all columns are strings only\n",
    "\t\t#df.to_excel(\"./output/crm_contact_with_row_no.xlsx\")\n",
    "\n",
    "\t# Clean text of all dataframe text data one time\n",
    "\tdf = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\n",
    "\t## Add New Columns to Add into original Dataframe\n",
    "\tdf.insert(0,const.COL_DUP_GROUP_ID,999999)\n",
    "\tif const.COL_ID not in df.columns:\n",
    "\t\t# Add RowNum to DataFrame\n",
    "\t\tdf[const.COL_ID] = range(0+1,len(df)+1)\n",
    "\tdf.insert(2,const.COL_ERROR_RATE,100.00)\n",
    "\tdf[const.COL_FUZZ_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_LEVENSHTEIN_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_JARO_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_DUP_ROW_GROUP] = \"\"\n",
    "\treturn df\n",
    "\n",
    "def save_file(df: any, file_path : str, ext:str):\n",
    "\tif ext.lower() == \".xlsx\":\n",
    "\t\tdf.to_excel(file_path,index=False)\n",
    "\telse:\n",
    "\t\tdf.to_csv(file_path,index=False)\n",
    "\n",
    "def load_file(file_path : str, ext:str):\n",
    "\tif ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path,encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path) \n",
    "\treturn df\n",
    "\n",
    "def process_contact_duplicator(file_path : str):\n",
    "\tstart_time = time.time()\t\n",
    "\ttry:\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\treturn\n",
    "\t\tname, ext = helper.get_file_name_and_extension(file_path)\n",
    "\t\tif not (ext.lower() == \".csv\" or ext.lower() == \".xlsx\"):\n",
    "\t\t\treturn \n",
    "\t\t# Step 1\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" +  name + \"_org\"+ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\tdf = read_file_prep_dataframe(file_path,ext)\n",
    "\t\t\tsave_file(df,file_out,ext)\n",
    "\t\telse:\n",
    "\t\t\tdf = load_file(file_out,ext)\n",
    "\t\t\t#return df\n",
    "\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" +  name + \"_dup\"+ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\t#Step 2\n",
    "\t\t\tdf_dup = find_dup_row_by_company_website(df)\n",
    "\t\t\t#Step 3\n",
    "\t\n",
    "\t\t\tdf = assign_dup_row_groups(df,df_dup)\n",
    "\t\t\tdf.to_csv(\"Output_1.csv\",date_format=str)\n",
    "\t\t\tdf.to_excel(\"Output_1.xlsx\")\n",
    "\t\t\tsave_file(df,file_out,ext)\n",
    "\t\t\treturn df\n",
    "\t\n",
    "\t\t# #Step 4 Assing the Keep / Duplicate Flag to each row based on research.\n",
    "\t\t# df = assign_keep_duplicate_flag(df)\n",
    "\n",
    "\t\t# file_out = const.OUTPUT_PATH + \"/\" +  name + \"_with_flag\"+ext\n",
    "\t\t# logger.info(f\"Saving file with duplicate groups and Keep delete flaf {file_out}\")\n",
    "\t\t# save_file(df,file_out,ext)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_group(row, row_to_group, group_id):\n",
    "\tif row in row_to_group:\n",
    "\t\t\treturn row_to_group[row]\n",
    "\trow_to_group[row] = group_id\n",
    "\treturn group_id\n",
    "def union_groups(row1, row2, row_to_group, group_id):\n",
    "\tgroup1 = find_group(row1, row_to_group, group_id)\n",
    "\tgroup2 = find_group(row2, row_to_group, group_id)\n",
    "\tif group1 != group2:\n",
    "\t\t\tfor row in row_to_group:\n",
    "\t\t\t\t\tif row_to_group[row] == group2:\n",
    "\t\t\t\t\t\t\trow_to_group[row] = group1\n",
    "\n",
    "def assign_group_ids(pairs):\n",
    "\trow_to_group = {}\n",
    "\tgroup_id = 1\n",
    "\n",
    "\tfor src_row, trg_row in pairs:\n",
    "\t\t\tif src_row not in row_to_group and trg_row not in row_to_group:\n",
    "\t\t\t\t\trow_to_group[src_row] = group_id\n",
    "\t\t\t\t\trow_to_group[trg_row] = group_id\n",
    "\t\t\t\t\tgroup_id += 1\n",
    "\t\t\telif src_row in row_to_group and trg_row not in row_to_group:\n",
    "\t\t\t\t\trow_to_group[trg_row] = row_to_group[src_row]\n",
    "\t\t\telif trg_row in row_to_group and src_row not in row_to_group:\n",
    "\t\t\t\t\trow_to_group[src_row] = row_to_group[trg_row]\n",
    "\t\t\telse:\n",
    "\t\t\t\t\tunion_groups(src_row, trg_row, row_to_group, group_id)\n",
    "\n",
    "\t# Ensure each component has a unique group ID\n",
    "\tunique_groups = {}\n",
    "\tcurrent_group_id = 1\n",
    "\tfor row in row_to_group:\n",
    "\t\t\told_group_id = row_to_group[row]\n",
    "\t\t\tif old_group_id not in unique_groups:\n",
    "\t\t\t\t\tunique_groups[old_group_id] = current_group_id\n",
    "\t\t\t\t\tcurrent_group_id += 1\n",
    "\t\t\trow_to_group[row] = unique_groups[old_group_id]\n",
    "\n",
    "\treturn row_to_group\n",
    "\n",
    "def assign_dup_row_groups(df,dup_df):\n",
    "\tdup_df = dup_df.sort_values(by=[\"src_row_no\"])\t\t\n",
    "\n",
    "\t# This will give Group ID assign to each row\t\n",
    "\tgroup_ids = assign_group_ids(list(dup_df[['src_row_no', 'trg_row_no']].itertuples(index=False,name=None)))\n",
    "\t\n",
    "\t# In this Loop If any two record qualify or three records, One Record data is missing.\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\t\t\n",
    "\t\tif df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0] == 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],group_ids[row[const.COL_SOURCE_ID]],str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],group_ids[row[const.COL_TARGET_ID]],str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\t\n",
    "\t#Loop Throuh again with Dup Record for any Source Or Target Row is missing For Group ID Update\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\tif grp_id != 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],grp_id,str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_TARGET_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],grp_id,str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = process_contact_duplicator(\"./data/Company_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value['row_group'] =value['row_group'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.to_csv(\"output_2.csv\",index=False,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dup_rows = []\n",
    "df_dup = df.groupby([\"Link\"], as_index=False).agg(count=(\"Link\", 'count'))\n",
    "df_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "for id, row in df_dup.iterrows():\n",
    "\tdf_flt = df[df[\"Link\"] == row[\"Link\"]]\n",
    "\tprint(df_flt)\n",
    "\tdf_flt = df_flt.sort_values(by=[\"Company Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value =find_dup_row_by_sequence(df=df_flt,sort_col=\"Company Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicat = pd.DataFrame(value)\n",
    "duplicat.to_csv(\"OutputFiles.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_idx in range(len(df_flt)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "print(src_idx)\n",
    "src_row[\"Link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.settings.constants as const\n",
    "def find_dup_row_by_sequence(df,sort_col):\n",
    "\tdf = df.sort_values(by=[sort_col])\n",
    "\tlist_dup_rows = []\n",
    "\t# Now Iterate through Each record and try to find out Group\n",
    "\tfor src_idx in range(len(df)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "\t\tfor trg_idx in range(len(df)):\n",
    "\t\t\ttrg_row = df.iloc[trg_idx]\n",
    "\t\t\t#logger.info(f\"source index {src_idx} ,target index : {trg_idx}\")\n",
    "\t\t\tif trg_idx > src_idx:\n",
    "\t\t\t\tdup_grp = {}\n",
    "\t\t\t\tdup_grp[const.COL_SOURCE_ID] = src_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[const.COL_TARGET_ID] = trg_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[\"src_Link\"] = str(src_row[\"Link\"])\n",
    "\t\t\t\tdup_grp[\"trg_Link\"] = str(trg_row[\"Link\"])\n",
    "\t\t\t\tid, rsp = compare_rows(src_row,trg_row)\n",
    "\t\t\t\t#logger.info(id, rsp)\n",
    "\t\t\t\tdup_grp[const.COL_FUZZ_SIMILARITY] = float(rsp[\"f\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_LEVENSHTEIN_SIMILARITY] = float(rsp[\"l\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_JARO_SIMILARITY] = float(rsp[\"j\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_ERROR_RATE] = float(rsp[\"e\" + str(id)])\n",
    "\t\t\t\tif dup_grp[const.COL_ERROR_RATE] <= const.MAX_ERROR_RATE:\n",
    "\t\t\t\t\tlist_dup_rows.append(dup_grp)\n",
    "\treturn list_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/Company_Data_1.1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If website does not exist, Need to fetch website first and then Deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import jellyfish as jfish\n",
    "import Levenshtein\n",
    "def compare_rows(src_row,trg_row):\n",
    "\t\"\"\"\n",
    "\t\tBased On Compare Columns, Calculate Levenstine, Jaro Winkle and Fuzzy raito and select\n",
    "\t\twhere Error rate lowest or similarity ratio is higher\n",
    "\t\"\"\"\n",
    "\tcomparesion_list = []\n",
    "\tid = 0\n",
    "\tmin_error = 100.0\n",
    "\tmin_error_id = -1\n",
    "\tCOMPARE_COLUMNS=[{ \"name\" : \"Rule1\", \"src_cols\":[\"Clean_Company\",], \"trg_cols\" : [\"Clean_Company\",]},\n",
    "\t\t\t\t  {\"name\" : \"Rule2\", \"src_cols\":[\"Clean_Website\"],\"trg_cols\" : [\"Clean_Website\"]}]\n",
    "\tfor comp_columns in COMPARE_COLUMNS:\n",
    "\t\tstr_src = get_col_value(src_row,comp_columns['src_cols'])\n",
    "\t\tstr_trg = get_col_value(trg_row,comp_columns['trg_cols'])\n",
    "\t\tres = {}\n",
    "\t\tres[\"name\"] = comp_columns[\"name\"]\n",
    "\t\tres[\"f\" + str(id)] = get_fuzzy_similarity(str_src,str_trg) \n",
    "\t\tres[\"l\" + str(id)] = get_levenshtein_similarity(str_src,str_trg)\n",
    "\t\tres[\"j\" + str(id)] = get_jaro_winkler_similarity(str_src,str_trg)\n",
    "\t\t#logger.info(f\"src val : {str_src}, trg val : {str_trg}, fuzz : {res['f' + str(id)] }, levst : {res['l' + str(id)]}, jero : {res['j' + str(id)]}\")\n",
    "\t\tres[\"e\" + str(id)] =  (1.0-(float(res[\"f\" + str(id)]) + float(res[\"l\" + str(id)]) + float(res[\"j\" + str(id)]) )/3)*100\n",
    "\t\tif ( float(res[\"e\" + str(id)]) < min_error):\n",
    "\t\t\tmin_error = res[\"e\" + str(id)]\n",
    "\t\t\tmin_error_id = id\n",
    "\t\tcomparesion_list.append(res)\n",
    "\t\tid += 1\t\n",
    "\t#logger.info(f\"min error rate : {min_error}, min error id : {min_error_id}\")\t\n",
    "\treturn min_error_id, comparesion_list[min_error_id]\n",
    "\n",
    "def get_fuzzy_similarity(str_src,str_trg):\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn (fuzz.token_set_ratio(str_src, str_trg)/100)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "def get_levenshtein_similarity(str_src,str_trg):\n",
    "\t\"\"\"\n",
    "\tLevenshtein distance measures the minimum number of single-character edits required to change one string into another.\n",
    "\t\"\"\"\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn Levenshtein.ratio(str_src, str_trg)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "def get_jaro_winkler_similarity(str_src,str_trg):\n",
    "\tif  (len(str_src) != 0 and len(str_trg) != 0):\n",
    "\t\treturn jfish.jaro_winkler_similarity(str_src, str_trg)\n",
    "\telse:\n",
    "\t\treturn 0.0\t\n",
    "\n",
    "def get_col_value(row : any,col_list : list) -> str:\n",
    "\tval_list = []\n",
    "\tfor col in col_list:\n",
    "\t\tval_list.append(str(row[col]).strip())\n",
    "\treturn \" \".join(val_list)\n",
    "\n",
    "import src.settings.constants as const\n",
    "def find_dup_row_by_sequence(df,sort_col):\n",
    "\tdf = df.sort_values(by=[sort_col])\n",
    "\tlist_dup_rows = []\n",
    "\t# Now Iterate through Each record and try to find out Group\n",
    "\tfor src_idx in range(len(df)):\n",
    "\t\tsrc_row = df.iloc[src_idx]\n",
    "\t\tfor trg_idx in range(len(df)):\n",
    "\t\t\ttrg_row = df.iloc[trg_idx]\n",
    "\t\t\t#logger.info(f\"source index {src_idx} ,target index : {trg_idx}\")\n",
    "\t\t\tif trg_idx > src_idx:\n",
    "\t\t\t\tdup_grp = {}\n",
    "\t\t\t\tdup_grp[const.COL_SOURCE_ID] = src_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[const.COL_TARGET_ID] = trg_row[const.COL_ID]\n",
    "\t\t\t\tdup_grp[\"src_Company Name\"] = str(src_row[\"Clean_Company\"])\n",
    "\t\t\t\tdup_grp[\"trg_Company Name\"] = str(trg_row[\"Clean_Company\"])\n",
    "\t\t\t\tid, rsp = compare_rows(src_row,trg_row)\n",
    "\t\t\t\t#logger.info(id, rsp)\n",
    "\t\t\t\tdup_grp[const.COL_FUZZ_SIMILARITY] = float(rsp[\"f\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_LEVENSHTEIN_SIMILARITY] = float(rsp[\"l\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_JARO_SIMILARITY] = float(rsp[\"j\" + str(id)])\n",
    "\t\t\t\tdup_grp[const.COL_ERROR_RATE] = float(rsp[\"e\" + str(id)])\n",
    "\t\t\t\tif dup_grp[const.COL_ERROR_RATE] <= const.MAX_ERROR_RATE:\n",
    "\t\t\t\t\tlist_dup_rows.append(dup_grp)\n",
    "\treturn list_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dup_row_by_company_website(df,col_first_name=\"Clean_Company\"):\n",
    "\tlist_dup_rows = []\n",
    "\tdf_dup = df.groupby([\"FL_Comp_name\"], as_index=False).agg(count=(\"FL_Comp_name\", 'count'))\n",
    "\tdf_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "\tfor id, row in df_dup.iterrows():\n",
    "\t\tdf_flt = df[df[\"FL_Comp_name\"] == row[\"FL_Comp_name\"]]\n",
    "\t\tdf_flt = df_flt.sort_values(by=[col_first_name])\n",
    "\t\tlist_dup_rows.extend(find_dup_row_by_sequence(df=df_flt,sort_col=col_first_name))\n",
    "\t\n",
    "\t# # Now Iterate through dup Rows and If last name matching Error rate is higher than threshold,\n",
    "\t# # Remove those rows from List.\n",
    "\t# list_dup_rows = remove_dup_rows_by_lastname_match_similarity(list_dup_rows)\n",
    "\t\n",
    "\tdf_dup_rows = pd.DataFrame.from_dict(list_dup_rows)\n",
    "\treturn df_dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_company_name(company_name):\n",
    "    # Define keywords to be removed\n",
    "    keywords = ['LLP', 'LLC', 'INC', 'PVT', 'LTD', 'CORP', 'CO']\n",
    "    # Create a regex pattern to match any of the keywords\n",
    "    pattern = r'\\b(?:' + '|'.join(keywords) + r')\\b\\.?'\n",
    "    # Substitute the keywords with an empty string\n",
    "    cleaned_name = re.sub(pattern, '', company_name, flags=re.IGNORECASE)\n",
    "    # Remove extra spaces\n",
    "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip()\n",
    "    return cleaned_name\n",
    "\n",
    "def remove_https(url):\n",
    "    # Check if the URL starts with \"https://\"\n",
    "    cleaned_url = re.sub(r'^(http://|https://)?(www\\.)?', '', url)\n",
    "    return cleaned_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from src.util import helper\n",
    "import re\n",
    "from src.settings import constants as const\n",
    "from src.util import helper \n",
    "import pandas as pd\n",
    "\n",
    "from src.webscrapper.webscrapper import get_company_website\n",
    "\n",
    "def fill_company_website(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if not row['Website']:  # Check if the website is empty\n",
    "            rsp = get_company_website(row['Company'])\n",
    "\t\t\t\n",
    "            if (get_fuzzy_similarity(row['Company'].strip().lower(),rsp[\"Link\"].strip().lower())*100) >=20:\n",
    "                df.at[index, 'Website'] = rsp['Link']\n",
    "                df.at[index, 'Company'] = rsp['Company_name']\n",
    "            else:\n",
    "                df.at[index, 'Website'] = \"Not Found\"           \n",
    "    return df\n",
    "def read_file_prep_dataframe(file_path, file_ext):\n",
    "\tif file_ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path,encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path) \n",
    "\n",
    "\tdf = df.fillna(\"\") # Fill Empty value as all columns are strings only\n",
    "\t\t#df.to_excel(\"./output/crm_contact_with_row_no.xlsx\")\n",
    "\n",
    "\t# Clean text of all dataframe text data one time\n",
    "\tdf = df.map(lambda x: helper.clean_contact_data(x) if isinstance(x, str) else x)\n",
    "\tprint(df.head())\n",
    "\tdf = fill_company_website(df)\n",
    "\tdf['Clean_Company']= df[\"Company\"].apply(clean_company_name)\n",
    "\tdf[\"Clean_Website\"] = df[\"Website\"].apply(remove_https)\n",
    "\n",
    "\t## Add New Columns to Add into original Dataframe\n",
    "\tdf.insert(0,const.COL_DUP_GROUP_ID,999999)\n",
    "\tif const.COL_ID not in df.columns:\n",
    "\t\t# Add RowNum to DataFrame\n",
    "\t\tdf[const.COL_ID] = range(0+1,len(df)+1)\n",
    "\tdf.insert(2,const.COL_ERROR_RATE,100.00)\n",
    "\tdf[const.COL_FUZZ_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_LEVENSHTEIN_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_JARO_SIMILARITY] = 0.0\n",
    "\tdf[const.COL_DUP_ROW_GROUP] = \"\"\n",
    "\t\n",
    "\tdf['FL_Comp_name'] = df['Company'].apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "\treturn df\n",
    "\n",
    "def save_file(df: any, file_path : str, ext:str):\n",
    "\tif ext.lower() == \".xlsx\":\n",
    "\t\tdf.to_excel(file_path,index=False)\n",
    "\telse:\n",
    "\t\tdf.to_csv(file_path,index=False)\n",
    "\n",
    "def load_file(file_path : str, ext:str):\n",
    "\tif ext.lower() == \".csv\":\n",
    "\t\tdf = pd.read_csv(file_path,encoding='cp1252')\n",
    "\telse:\n",
    "\t\tdf = pd.read_excel(file_path) \n",
    "\treturn df\n",
    "\n",
    "def process_contact_duplicator(file_path : str):\n",
    "\tstart_time = time.time()\t\n",
    "\ttry:\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\treturn\n",
    "\t\tname, ext = helper.get_file_name_and_extension(file_path)\n",
    "\t\tif not (ext.lower() == \".csv\" or ext.lower() == \".xlsx\"):\n",
    "\t\t\treturn \n",
    "\t\t# Step 1\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" +  name + \"_org\"+ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\tdf = read_file_prep_dataframe(file_path,ext)\n",
    "\t\t\tsave_file(df,file_out,ext)\n",
    "\t\telse:\n",
    "\t\t\tdf = load_file(file_out,ext)\n",
    "\t\t\t#return df\n",
    "\t\t\n",
    "\t\tdf[\"Domain\"] = df[\"Clean_Website\"].apply(lambda x: x.split('.')[0])\n",
    "\t\t\n",
    "\t\tfile_out = const.OUTPUT_PATH + \"/\" +  name + \"_dup\"+ext\n",
    "\t\tif not os.path.exists(file_out):\n",
    "\t\t\t#Step 2\n",
    "\t\t\tprint(df.head())\n",
    "\t\t\tdf_dup = find_dup_row_by_company_website(df)\n",
    "\t\t\t#Step 3\n",
    "\t\n",
    "\t\t\tdf = assign_dup_row_groups(df,df_dup)\n",
    "\t\t\tdf.to_csv(\"Output_1.csv\",date_format=str)\n",
    "\t\t\tdf.to_excel(\"Output_1.xlsx\",index=False)\n",
    "\t\t\tsave_file(df,file_out,ext)\n",
    "\t\t\treturn df\n",
    "\t\n",
    "\t\t# #Step 4 Assing the Keep / Duplicate Flag to each row based on research.\n",
    "\t\t# df = assign_keep_duplicate_flag(df)\n",
    "\n",
    "\t\t# file_out = const.OUTPUT_PATH + \"/\" +  name + \"_with_flag\"+ext\n",
    "\t\t# logger.info(f\"Saving file with duplicate groups and Keep delete flaf {file_out}\")\n",
    "\t\t# save_file(df,file_out,ext)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_group(row, row_to_group, group_id):\n",
    "\tif row in row_to_group:\n",
    "\t\t\treturn row_to_group[row]\n",
    "\trow_to_group[row] = group_id\n",
    "\treturn group_id\n",
    "def union_groups(row1, row2, row_to_group, group_id):\n",
    "\tgroup1 = find_group(row1, row_to_group, group_id)\n",
    "\tgroup2 = find_group(row2, row_to_group, group_id)\n",
    "\tif group1 != group2:\n",
    "\t\t\tfor row in row_to_group:\n",
    "\t\t\t\t\tif row_to_group[row] == group2:\n",
    "\t\t\t\t\t\t\trow_to_group[row] = group1\n",
    "\n",
    "def assign_group_ids(pairs):\n",
    "\trow_to_group = {}\n",
    "\tgroup_id = 1\n",
    "\n",
    "\tfor src_row, trg_row in pairs:\n",
    "\t\t\tif src_row not in row_to_group and trg_row not in row_to_group:\n",
    "\t\t\t\t\trow_to_group[src_row] = group_id\n",
    "\t\t\t\t\trow_to_group[trg_row] = group_id\n",
    "\t\t\t\t\tgroup_id += 1\n",
    "\t\t\telif src_row in row_to_group and trg_row not in row_to_group:\n",
    "\t\t\t\t\trow_to_group[trg_row] = row_to_group[src_row]\n",
    "\t\t\telif trg_row in row_to_group and src_row not in row_to_group:\n",
    "\t\t\t\t\trow_to_group[src_row] = row_to_group[trg_row]\n",
    "\t\t\telse:\n",
    "\t\t\t\t\tunion_groups(src_row, trg_row, row_to_group, group_id)\n",
    "\n",
    "\t# Ensure each component has a unique group ID\n",
    "\tunique_groups = {}\n",
    "\tcurrent_group_id = 1\n",
    "\tfor row in row_to_group:\n",
    "\t\t\told_group_id = row_to_group[row]\n",
    "\t\t\tif old_group_id not in unique_groups:\n",
    "\t\t\t\t\tunique_groups[old_group_id] = current_group_id\n",
    "\t\t\t\t\tcurrent_group_id += 1\n",
    "\t\t\trow_to_group[row] = unique_groups[old_group_id]\n",
    "\n",
    "\treturn row_to_group\n",
    "\n",
    "def assign_dup_row_groups(df,dup_df):\n",
    "\tdup_df = dup_df.sort_values(by=[\"src_row_no\"])\t\t\n",
    "\n",
    "\t# This will give Group ID assign to each row\t\n",
    "\tgroup_ids = assign_group_ids(list(dup_df[['src_row_no', 'trg_row_no']].itertuples(index=False,name=None)))\n",
    "\t\n",
    "\t# In this Loop If any two record qualify or three records, One Record data is missing.\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\t\t\n",
    "\t\tif df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0] == 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],group_ids[row[const.COL_SOURCE_ID]],str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],group_ids[row[const.COL_TARGET_ID]],str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\t\n",
    "\t#Loop Throuh again with Dup Record for any Source Or Target Row is missing For Group ID Update\n",
    "\tfor src_idx in range(len(dup_df)):\n",
    "\t\trow = dup_df.iloc[src_idx]\n",
    "\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_SOURCE_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\tif grp_id != 999999:\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_TARGET_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],grp_id,str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\t\telse:\n",
    "\t\t\tgrp_id = df[df[const.COL_ID] == row[const.COL_TARGET_ID]][const.COL_DUP_GROUP_ID].values[0]\n",
    "\t\t\tdf.loc[df[const.COL_ID] == row[const.COL_SOURCE_ID], [const.COL_FUZZ_SIMILARITY, const.COL_LEVENSHTEIN_SIMILARITY, \\\n",
    "\t\t\t\t\tconst.COL_JARO_SIMILARITY,const.COL_ERROR_RATE,const.COL_DUP_GROUP_ID,const.COL_DUP_ROW_GROUP]] \\\n",
    "\t\t\t\t\t= [row[const.COL_FUZZ_SIMILARITY],row[const.COL_LEVENSHTEIN_SIMILARITY],row[const.COL_JARO_SIMILARITY], \\\n",
    "\t\t\t\t\trow[const.COL_ERROR_RATE],grp_id,str(row[const.COL_SOURCE_ID])+\"-\"+str(row[const.COL_TARGET_ID])]\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_contact_duplicator(\"./data/Compant_Data_1.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./output/Company_Data_org.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dup_rows = []\n",
    "df_dup = df.groupby([\"fname_Company\"], as_index=False).agg(count=(\"fname_Company\", 'count'))\n",
    "df_dup = df_dup[df_dup[\"count\"] > 1]\n",
    "for id, row in df_dup.iterrows():\n",
    "\tdf_flt = df[df[\"fname_Company\"] == row[\"fname_Company\"]]\n",
    "\tdf_flt = df_flt.sort_values(by=[\"Company\"])\n",
    "\tdf_dup_lname = df_flt.groupby([\"Company\"], as_index=False).agg(count=(\"Company\", 'count'))\n",
    "\tdf_dup_lname = df_dup_lname[df_dup_lname[\"count\"] > 1]\n",
    "\tfor id1, row in df_dup_lname.iterrows():\n",
    "\t\tdf_flt_lname = df_flt[df_flt['Company'] == row['Company']]\n",
    "\t\tdf_flt_lname = df_flt_lname.sort_values(by=[\"Company\"])\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flt_lname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/Compant_Data_1.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.duplicate_finder.Company_Duplicate_finder import process_company_duplicator\n",
    "df = process_company_duplicator(\"./data/Compant_Data_1.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.webscrapper.webscrapper import get_company_website\n",
    "\n",
    "def fill_company_website(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if not row['Website']:  # Check if the website is empty\n",
    "            rsp = get_company_website(row['Company'])\n",
    "            if rsp['Link'] !=\"Not Found\":\n",
    "                df.at[index, 'Website'] = rsp['Link']\n",
    "                df.at[index, 'Company'] = rsp['Company_name']\n",
    "            else:\n",
    "                df.at[index, 'Website'] = \"Not Found\"\n",
    "          \n",
    "               \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_company_website(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.webscrapper.webscrapper import get_company_website\n",
    "get_company_website(\"BDO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.webscrapper.webscrapper import get_google_api_resposne_search_items\n",
    "rsp = get_google_api_resposne_search_items(\"BDO +website+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_company_website(\"beyond markets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp['data'][0].get('snippet') is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_company_website(\"ab bernstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_google_api_resposne_search_items(\"www.bernsteinresearch.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_google_api_resposne_search_items(\"www.akemona.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_google_api_resposne_search_items(\"www.withbanner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_company_website(\"Banner Technologies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company Name and title will Different \n",
    "get_google_api_resposne_search_items(\"Banner Technologies Website\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# website Is Not Working still it will give me the result\n",
    "get_google_api_resposne_search_items(\"http://caplab.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrong Website\n",
    "get_company_website(\"CapLab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_company_name(company_name):\n",
    "    # Define keywords to be removed\n",
    "    keywords = ['LLP', 'LLC', 'INC', 'PVT', 'LTD', 'CORP', 'CO']\n",
    "    # Create a regex pattern to match any of the keywords\n",
    "    pattern = r'\\b(?:' + '|'.join(keywords) + r')\\b\\.?'\n",
    "    # Substitute the keywords with an empty string\n",
    "    cleaned_name = re.sub(pattern, '', company_name, flags=re.IGNORECASE)\n",
    "    # Remove extra spaces\n",
    "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip()\n",
    "    return cleaned_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_https(url):\n",
    "    # Check if the URL starts with \"https://\"\n",
    "    if url.startswith(\"https://\"):\n",
    "        # Remove \"https://\" from the URL\n",
    "        return url[8:]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Company']= df['Company'].apply(clean_company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"Output_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action\"] = \"\"\n",
    "df[\"Real_Name\"] =\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_exact_duplicate_flag(df : any):\n",
    "\tdf_flt = df[df['dup_group_id'] != 999999]\n",
    "\tdf_grp = df_flt.groupby([\"dup_group_id\",\"Clean_Website\"], as_index=False).agg(count=(\"dup_group_id\", 'count'))\n",
    "\tlist_exact_dup = df_grp[df_grp[\"count\"] > 1][\"dup_group_id\"].unique().tolist()\n",
    "\tfor exact_dup_id in list_exact_dup:\n",
    "\t\tdf_tmp = df[df[\"dup_group_id\"] == exact_dup_id]\n",
    "\t\tdf_tmp = df_tmp.sort_values(by=[\"Company\"], ascending=False)\t\t\n",
    "\t\tfor idx in range(len(df_tmp)):\n",
    "\t\t\trow = df_tmp.iloc[idx]\n",
    "\t\t\tif idx == 0:\n",
    "\t\t\t\tflag = \"Keep\"\n",
    "\t\t\telse:\n",
    "\t\t\t\tflag = \"Duplicate\"\t\n",
    "\t\t\tdf.loc[(df[\"dup_group_id\"] == exact_dup_id) & (df[\"RowNo\"] == row[\"RowNo\"]), [\"action\"]] = [flag]\n",
    "\treturn df, list_exact_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,list_exact_dup = assign_exact_duplicate_flag(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flt = df[df['dup_group_id'] != 999999]\n",
    "df_grp = df_flt.groupby([\"dup_group_id\",\"Company\",\"Website\"],as_index=False).agg(count=(\"dup_group_id\", 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp[\"Website\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.webscrapper.webscrapper import get_website_robot_file\n",
    "robot_file = get_website_robot_file( \"www.abacasexchange.com\",\"abacaschange\")\n",
    "robot_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.webscrapper.webscrapper import get_google_api_resposne_search_items\n",
    "get_google_api_resposne_search_items(\"www.abacaange.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.settings.constants as const\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "import src.webscrapper.webscrapper as wsc\n",
    "def search_contact_information(Website,Company):\n",
    "    rsp = {}\n",
    "    rsp[\"msg\"] = \"\"\n",
    "    rsp[\"Company_name\"] = const.NOT_FOUND\n",
    "    robot_file = wsc.get_website_robot_file( Website,Company)\n",
    "    page_rsp = wsc.get_page_content(Website,robot_file)\n",
    "    if page_rsp[\"is_success\"]:\n",
    "        phrases = re.findall(r'\\b[A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)*\\b', page_rsp[\"page_text\"])\n",
    "        similar_phrases = process.extract(Company, phrases, scorer=fuzz.token_sort_ratio, limit=1)\n",
    "        rsp[\"Company_name\"]= similar_phrases[0][0]\n",
    "    \n",
    "    return rsp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_url(url):\n",
    "    if not url.startswith('https://'):\n",
    "        # If it doesn't, add 'https://'\n",
    "        url = 'https://' + url\n",
    "    if not url.startswith(\"https://www.\"):\n",
    "        url = \"https://www.\" + url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.webscrapper.webscrapper as wsc\n",
    "import src.settings.constants as const\n",
    "import re\n",
    "def search_Company_information(Website, Company):\n",
    "    rsp = {}\n",
    "    rsp[\"msg\"] = \"\"\n",
    "    rsp[\"Company_name\"] = const.NOT_FOUND\n",
    "    website_valid = validate_url(Website)\n",
    "    robot_file = wsc.get_website_robot_file( website_valid, Company)\n",
    "    page_rsp = wsc.get_page_content(website_valid, robot_file)\n",
    "    \n",
    "    if page_rsp[\"is_success\"]:\n",
    "        page_text = str(page_rsp[\"page_text\"])\n",
    "        str_input=re.sub(r'[^\\x20-\\x7E]', '', page_text)\n",
    "        str_input=re.sub(r'\\s+', ' ', str_input)\n",
    "        str_input = str_input.strip().lower()  # Ensure proper indentation here\n",
    "        if str_input.find(Company.lower()) > -1:  # Comparing in lowercase for consistency\n",
    "            rsp[\"Company_name\"] = Company\n",
    "    return rsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_phrases[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_url(url):\n",
    "    if not url.startswith('https://'):\n",
    "        # If it doesn't, add 'https://'\n",
    "        url = 'https://' + url\n",
    "    if not url.startswith(\"https://www.\"):\n",
    "        url = \"https://www.\" + url\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_probable_duplicate_flag(df : any, list_exact_dup : list[int]):\n",
    "\t# Remove Exact Duplicate rows from Dataframe\n",
    "\tdf_flt = df[df['dup_group_id'] != 999999]\n",
    "\tfor exact_dup_id in list_exact_dup:\n",
    "\t\tdf_flt = df_flt[df_flt['dup_group_id'] != exact_dup_id]\n",
    "\t# Sort by Dup Group ID\n",
    "\tdf_flt = df_flt.sort_values(by=[\"dup_group_id\"])\n",
    "\tfor id, row in df_flt.iterrows():\n",
    "\t\trsp=search_Company_information(row['Website'],row[\"Company\"])\n",
    "\t\tif rsp['Company_name'] != \"Not found\":\n",
    "\t\t\tdf.at[id, 'Real_Name'] = rsp['Company_name']\n",
    "\t\t\tdf.at[id,\"action\"] = \"Keep\"\n",
    "\t\telse:\n",
    "\t\t\tdf.at[id, 'action'] =\"Not Sure\"\n",
    "\t\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assign_probable_duplicate_flag(df,list_exact_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Flag_Assigned_Duplicat_records_2.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "Apple Inc. is a technology company known for its iPhone and Mac products. \n",
    "Microsoft Corporation develops Windows and Office software. \n",
    "Amazon is an e-commerce giant. \n",
    "Google, under Alphabet Inc., dominates the search engine market.\n",
    "\"\"\"\n",
    "\n",
    "phrases = re.findall(r'\\b[A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)*\\b', text)\n",
    "print(phrases)\n",
    "\n",
    "# Provided company name\n",
    "company_name = \"Apple Inc.\"\n",
    "\n",
    "\n",
    "similar_phrases = process.extract(company_name, phrases, scorer=fuzz.token_sort_ratio, limit=1)\n",
    "\n",
    "\n",
    "print(f\"Similar phrases to '{company_name}':\")\n",
    "for phrase, similarity in similar_phrases:\n",
    "    print(f\"{phrase}: {similarity}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"\"\"\n",
    "Apple Inc. is a technology company known for its iPhone and Mac products. \n",
    "Microsoft Corporation develops Windows and Office software. \n",
    "Amazon is an e-commerce giant. \n",
    "Google, under Alphabet Inc., dominates the search engine market.\n",
    "\"\"\"\n",
    "re.findall(text,\"Microsoft Corporation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b' + re.escape(\"Apple INC.\") + r'\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_apple(text):\n",
    "    # Define the company name to search for\n",
    "    company = \"Apple Inc.\"\n",
    "    \n",
    "    # Use word boundaries (\\b) to ensure exact matches\n",
    "    pattern = r'\\b' + re.escape(company) + r'\\b'\n",
    "    \n",
    "    # Find matches using re.findall\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Return the company name if it is found, otherwise return None\n",
    "    return company if matches else None\n",
    "\n",
    "# Example usage\n",
    "page_text = \"\"\"Apple Inc. is a technology company known for its iPhone and Mac products. \n",
    "Microsoft Corporation develops Windows and Office software. \n",
    "Amazon is an e-commerce giant. \n",
    "Google, under Alphabet Inc., dominates the search engine market.\"\"\"\n",
    "\n",
    "matched_company = find_apple(page_text)\n",
    "print(matched_company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text = \"\"\"Apple Inc. is a technology company known for its iPhone and Mac products. \n",
    "Microsoft Corporation develops Windows and Office software. \n",
    "Amazon is an e-commerce giant. \n",
    "Google, under Alphabet Inc., dominates the search engine market.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text.find(\"Ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.webscrapper.webscrapper as wsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc.get_page_content(\"www.acryliccap.com\",\"not define\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc.get_page_content(\"https://www.aerocapitalsolutions.com\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc.get_page_content(\"www.beyondcapitalmarkets.com\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc.get_page_content(\"https://www.bluetradingsystems.com\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc.get_page_content(\"www.capconcorp.com\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc.get_page_content(\"http://caplab.ai\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = wsc.get_google_api_resposne_search_items(\"bids trading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_url(url):\n",
    "    if not url.startswith('https://'):\n",
    "        # If it doesn't, add 'https://'\n",
    "        url = 'https://' + url\n",
    "    if not url.startswith(\"https://www.\"):\n",
    "        url = \"https://www.\" + url\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_url(\"www.actualgroup.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.duplicate_finder.Company_Duplicate_finder as cdup\n",
    "df = cdup.process_company_duplicator(\"./data/Compant_Data_1.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Company_Data_1.2_with_flag_1.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.webscrapper.webscrapper as wsc\n",
    "rsp = wsc.get_google_api_resposne_search_items(\"legal name of company 'BTIG'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTIG, LLC\n",
      "https://www.sec.gov/Archives/edgar/data/1178937/000117893720000007/rePublic.pdf\n",
      "BTIG, LLC Dec 31, 2019 ... ADDRESS OF PRINCIPAL PLACE OF BUSINESS: (Do note use P.O. Box No.) 600 Montgomery Street 6 Floor. OFFICIAL USE ONLY. FIRM I.D. NO. San Francisco.\n",
      "Bartlett LLP\n",
      "https://www.btig.com/news/btig-adds-winston-kitchingham-to-legal-team/\n",
      "BTIG Adds Winston Kitchingham to Legal Team - BTIG May 23, 2022 ... Kitchingham began his career as a corporate lawyer at Simpson Thacher & Bartlett LLP and Dewey Ballantine LLP. BTIG Investment Banking and ...\n",
      "BTIG, LLC\n",
      "https://files.brokercheck.finra.org/firm/firm_122225.pdf\n",
      "BrokerCheck Report - BTIG, LLC Apr 28, 2017 ... This firm was formed in Delaware on 06/05/2002. CRD#. This section provides the brokerage firm's full legal name, \"Doing Business As\" name, ...\n",
      "BTIG, LLC\n",
      "https://brokercheck.finra.org/firm/summary/122225\n",
      "BTIG, LLC - BrokerCheck Brokerage Firm Regulated by FINRA (San Francisco district office) MAIN ADDRESS 350 BUSH STREET 9TH FLOOR SAN FRANCISCO, CA 94104 USA\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_legal_name(text):\n",
    "    pattern = r'\\b[A-Za-z0-9\\s,]+?\\s(Inc\\.|LLC|Ltd\\.|Corp\\.|Corporation|Co\\.|Limited|LLP|L\\.P\\.|PLC|GmbH|S\\.A\\.|Pty\\. Ltd\\.|BV)\\b'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "\n",
    "for item in rsp[\"data\"]:\n",
    "\tinput_text = item[\"title\"] + \" \" +  item[\"snippet\"]\n",
    "\tcname = extract_legal_name(input_text)\n",
    "\tif cname != \"Not found\":\n",
    "\t\tprint(cname)\n",
    "\t\tprint(item[\"link\"])\n",
    "\t\tprint(input_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
